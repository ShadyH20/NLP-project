{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adtfwaAwCPq0"
   },
   "source": [
    "### Installation & Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T04:58:38.113134Z",
     "iopub.status.busy": "2025-04-22T04:58:38.112884Z",
     "iopub.status.idle": "2025-04-22T04:58:44.962571Z",
     "shell.execute_reply": "2025-04-22T04:58:44.961890Z",
     "shell.execute_reply.started": "2025-04-22T04:58:38.113114Z"
    },
    "executionInfo": {
     "elapsed": 8793,
     "status": "ok",
     "timestamp": 1745185615542,
     "user": {
      "displayName": "Sherifa Hammoud",
      "userId": "01658453922136634541"
     },
     "user_tz": -120
    },
    "id": "V3oNJP0SCHgb",
    "outputId": "6688ee3a-7b98-44ea-95bf-2c753e0f02b5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T04:58:53.093246Z",
     "iopub.status.busy": "2025-04-22T04:58:53.092947Z",
     "iopub.status.idle": "2025-04-22T04:58:56.166153Z",
     "shell.execute_reply": "2025-04-22T04:58:56.165474Z",
     "shell.execute_reply.started": "2025-04-22T04:58:53.093205Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4383,
     "status": "ok",
     "timestamp": 1745185619928,
     "user": {
      "displayName": "Sherifa Hammoud",
      "userId": "01658453922136634541"
     },
     "user_tz": -120
    },
    "id": "CuBL8BGY-p7X",
    "outputId": "48291094-3214-4046-cf0f-79c542351876",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install torchtext==0.17.0 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T17:24:07.404867Z",
     "iopub.status.busy": "2025-04-22T17:24:07.404599Z",
     "iopub.status.idle": "2025-04-22T17:24:11.613892Z",
     "shell.execute_reply": "2025-04-22T17:24:11.613189Z",
     "shell.execute_reply.started": "2025-04-22T17:24:07.404847Z"
    },
    "executionInfo": {
     "elapsed": 5182,
     "status": "ok",
     "timestamp": 1745185625112,
     "user": {
      "displayName": "Sherifa Hammoud",
      "userId": "01658453922136634541"
     },
     "user_tz": -120
    },
    "id": "W9Q8pHZ2-HLZ",
    "outputId": "37046b77-2455-4716-966f-9afb71d80b00",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1+cu124\n",
      "transformers version: 4.51.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "# import torchtext\n",
    "import transformers\n",
    "\n",
    "print(f'torch version: {torch.__version__}')\n",
    "# print(f'torchtext version: {torchtext.__version__}')\n",
    "print(f'transformers version: {transformers.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNL7-BZQCIJM"
   },
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T17:35:31.967716Z",
     "iopub.status.busy": "2025-04-22T17:35:31.967367Z",
     "iopub.status.idle": "2025-04-22T17:35:53.280891Z",
     "shell.execute_reply": "2025-04-22T17:35:53.280215Z",
     "shell.execute_reply.started": "2025-04-22T17:35:31.967690Z"
    },
    "executionInfo": {
     "elapsed": 36508,
     "status": "ok",
     "timestamp": 1745185661622,
     "user": {
      "displayName": "Sherifa Hammoud",
      "userId": "01658453922136634541"
     },
     "user_tz": -120
    },
    "id": "t5Fdg-h4zPFE",
    "outputId": "042e504e-ae1d-4d0f-f082-47bf2a227fca",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n",
      "                             id          title  \\\n",
      "12947  56df7f0156340a1900b29c38  Oklahoma_City   \n",
      "12697  56df6bf35ca0a614008f9a00      Christian   \n",
      "\n",
      "                                                 context  \\\n",
      "12947  The Oklahoma School of Science and Mathematics...   \n",
      "12697  In the past, the Malays used to call the Portu...   \n",
      "\n",
      "                                                question  \\\n",
      "12947  Where is The Oklahoma School of Science and Ma...   \n",
      "12697                  What does the term refer to now?    \n",
      "\n",
      "                                                 answers  context_length  \n",
      "12947  {'text': ['Oklahoma City'], 'answer_start': [1...             151  \n",
      "12697  {'text': ['the modern Kristang creoles of Mala...             151  \n",
      "\n",
      "\n",
      "\n",
      "id                                         56df7f0156340a1900b29c38\n",
      "title                                                 Oklahoma_City\n",
      "context           The Oklahoma School of Science and Mathematics...\n",
      "question          Where is The Oklahoma School of Science and Ma...\n",
      "answers           {'text': ['Oklahoma City'], 'answer_start': [1...\n",
      "context_length                                                  151\n",
      "Name: 12947, dtype: object\n",
      "start_char $137\n",
      "end_char $150\n",
      "start_token $29\n",
      "end_token $30\n",
      "\n",
      "\n",
      "Original Answer: Oklahoma City\n",
      "Reconstructed Answer: oklahoma city\n",
      "\n",
      "\n",
      "\n",
      "id                                         56df6bf35ca0a614008f9a00\n",
      "title                                                     Christian\n",
      "context           In the past, the Malays used to call the Portu...\n",
      "question                          What does the term refer to now? \n",
      "answers           {'text': ['the modern Kristang creoles of Mala...\n",
      "context_length                                                  151\n",
      "Name: 12697, dtype: object\n",
      "start_char $111\n",
      "end_char $150\n",
      "start_token $27\n",
      "end_token $34\n",
      "\n",
      "\n",
      "Original Answer: the modern Kristang creoles of Malaysia\n",
      "Reconstructed Answer: the modern kristang creoles of malaysia\n",
      "Train set size: 16000\n",
      "Validation set size: 4000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from torchtext.vocab import GloVe, build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import itertools\n",
    "\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ## Load and preprocess dataset\n",
    "\n",
    "# dataset = load_dataset('squad')\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "# df = pd.DataFrame(dataset['train'])\n",
    "# df = df[['context', 'question', 'answers']]\n",
    "# df['answer_text'] = df['answers'].apply(lambda x: x['text'][0])\n",
    "# df['answer_start'] = df['answers'].apply(lambda x: x['answer_start'][0])\n",
    "# df = df.drop(columns=['answers'])\n",
    "\n",
    "# print(df.head(2))\n",
    "\n",
    "# # Use a subset\n",
    "# df['context_len'] = df['context'].apply(len)\n",
    "# df = df.sort_values(by='context_len').iloc[:10000].drop(columns=['context_len'])\n",
    "\n",
    "# # Tokenization\n",
    "# df['context_tok'] = df['context'].apply(lambda x: word_tokenize(x.lower()))\n",
    "# df['question_tok'] = df['question'].apply(lambda x: word_tokenize(x.lower()))\n",
    "# df['answer_tok'] = df['answer_text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "dataset = load_dataset('squad')\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Take the first 16k rows with the shortest context (train data)\n",
    "# train_data_old = dataset['train']\n",
    "# train_data_old = train_data_old.map(lambda example: {'context_length': len(example['context'])})\n",
    "# train_data_sorted = train_data_old.sort('context_length').select(range(16000))\n",
    "train_df = dataset['train'].to_pandas()\n",
    "train_df['context_length'] = train_df['context'].apply(len)\n",
    "train_df = train_df.sort_values('context_length').head(16000)\n",
    "\n",
    "# Take the first 4k rows with the shortest context (validation data)\n",
    "# val_data_old = dataset['validation']\n",
    "# val_data_old = val_data_old.map(lambda example: {'context_length': len(example['context'])})\n",
    "# val_data_sorted = val_data_old.sort('context_length').select(range(10000))\n",
    "val_df = dataset['validation'].to_pandas()\n",
    "val_df['context_length'] = val_df['context'].apply(len)\n",
    "val_df = val_df.sort_values('context_length').head(4000)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "test_tok = 2\n",
    "\n",
    "def tokenize_data(example):\n",
    "    # Tokenize the context and question\n",
    "    encoding = tokenizer(example['context'], example['question'], truncation=True, padding='max_length', max_length=512, return_offsets_mapping=True)\n",
    "    offsets = encoding['offset_mapping']\n",
    "    input_ids = encoding['input_ids']\n",
    "\n",
    "    # Convert the answer start index from character index to token index\n",
    "    start_char = example['answers']['answer_start'][0]\n",
    "    end_char = start_char + len(example['answers']['text'][0])\n",
    "\n",
    "    start_token = end_token = None\n",
    "    for idx, (start, end) in enumerate(offsets):\n",
    "        if start_token is None and start <= start_char < end:\n",
    "            start_token = idx\n",
    "        if end_token is None and start < end_char <= end:\n",
    "            end_token = idx\n",
    "\n",
    "    if start_token is None or end_token is None:\n",
    "        return None\n",
    "\n",
    "    encoding['start_positions'] = start_token\n",
    "    encoding['end_positions'] = end_token\n",
    "\n",
    "    reconstructed_answer = tokenizer.decode(input_ids[start_token:end_token + 1], skip_special_tokens=True)\n",
    "\n",
    "    global test_tok\n",
    "    if test_tok > 0:\n",
    "        test_tok = test_tok - 1\n",
    "        print('\\n\\n')\n",
    "        print(example)\n",
    "        print(f'start_char ${start_char}')\n",
    "        print(f'end_char ${end_char}')\n",
    "        print(f'start_token ${start_token}')\n",
    "        print(f'end_token ${end_token}')\n",
    "\n",
    "        print(f\"\\n\\nOriginal Answer: {example['answers']['text'][0]}\")\n",
    "        print(f\"Reconstructed Answer: {reconstructed_answer}\")\n",
    "\n",
    "    return encoding\n",
    "\n",
    "# Apply tokenization to both training and validation sets\n",
    "\n",
    "# tokenized_train = train_data_sorted.map(tokenize_data, remove_columns=train_data_sorted.column_names)\n",
    "# tokenized_train = tokenized_train.filter(lambda x: x is not None)\n",
    "\n",
    "# tokenized_val = val_data_sorted.map(tokenize_data, remove_columns=val_data_sorted.column_names)\n",
    "# tokenized_val = tokenized_val.filter(lambda x: x is not None)\n",
    "\n",
    "# train_encoded = train_df.apply(tokenize_data, axis=1).dropna()\n",
    "# val_encoded = val_df.apply(tokenize_data, axis=1).dropna()\n",
    "\n",
    "print(train_df.head(2))\n",
    "\n",
    "# Convert filtered DataFrames back into Hugging Face Datasets\n",
    "# train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "# val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "# # Apply tokenization using map (which expects dictionary input/output)\n",
    "# tokenized_train = train_dataset.map(tokenize_data, remove_columns=train_dataset.column_names)\n",
    "# tokenized_train = tokenized_train.filter(lambda x: x is not None)\n",
    "\n",
    "# tokenized_val = val_dataset.map(tokenize_data, remove_columns=val_dataset.column_names)\n",
    "# tokenized_val = tokenized_val.filter(lambda x: x is not None)\n",
    "\n",
    "# # Convert tokenized data to pandas DataFrames\n",
    "# train_encoded_df = tokenized_train.to_pandas()\n",
    "# val_encoded_df = tokenized_val.to_pandas()\n",
    "\n",
    "# train_data = train_data_sorted.map(tokenize_data, remove_columns=train_data_sorted.column_names)\n",
    "# train_data = train_data.filter(lambda x: x is not None)\n",
    "# val_data = val_data_sorted.map(tokenize_data, remove_columns=val_data_sorted.column_names)\n",
    "# val_data = val_data.filter(lambda x: x is not None)\n",
    "# Apply tokenization to both training and validation sets\n",
    "train_encoded_df = train_df.apply(tokenize_data, axis=1)\n",
    "train_encoded_df = train_encoded_df.dropna()\n",
    "train_encoded_df = train_encoded_df.reset_index(drop=True)\n",
    "val_encoded_df = val_df.apply(tokenize_data, axis=1)\n",
    "val_encoded_df = val_encoded_df.dropna()\n",
    "val_encoded_df = val_encoded_df.reset_index(drop=True)\n",
    "print(f\"Train set size: {len(train_encoded_df)}\")\n",
    "print(f\"Validation set size: {len(val_encoded_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T17:35:57.525146Z",
     "iopub.status.busy": "2025-04-22T17:35:57.524446Z",
     "iopub.status.idle": "2025-04-22T17:35:57.531569Z",
     "shell.execute_reply": "2025-04-22T17:35:57.530758Z",
     "shell.execute_reply.started": "2025-04-22T17:35:57.525120Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11600    [input_ids, token_type_ids, attention_mask, of...\n",
       "13825    [input_ids, token_type_ids, attention_mask, of...\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(train_df_sorted.head(4))\n",
    "# print(df.head(4))\n",
    "# tokenized_train2 = tokenized_train.select(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
    "# tokenized_train_focused = tokenized_train.remove_columns(['token_type_ids', 'offset_mapping'])\n",
    "# tokenized_val_focused = tokenized_val.remove_columns(['token_type_ids', 'offset_mapping'])\n",
    "\n",
    "# print(tokenized_train_focused)\n",
    "# print(tokenized_val_focused)\n",
    "\n",
    "train_encoded_df.sample(2)\n",
    "# print(val_encoded.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62xRU1SMCBBk"
   },
   "source": [
    "### Build Vocabulary & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T04:01:08.974766Z",
     "iopub.status.busy": "2025-04-22T04:01:08.974223Z",
     "iopub.status.idle": "2025-04-22T04:01:10.205928Z",
     "shell.execute_reply": "2025-04-22T04:01:10.205326Z",
     "shell.execute_reply.started": "2025-04-22T04:01:08.974743Z"
    },
    "executionInfo": {
     "elapsed": 1552,
     "status": "ok",
     "timestamp": 1745185663170,
     "user": {
      "displayName": "Sherifa Hammoud",
      "userId": "01658453922136634541"
     },
     "user_tz": -120
    },
    "id": "IwgQpkNozZdl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ## Build vocabulary\n",
    "\n",
    "# specials = ['<pad>', '<unk>']\n",
    "\n",
    "# def yield_tokens():\n",
    "#     for tokens in itertools.chain(df['context_tok'], df['question_tok'], df['answer_tok']):\n",
    "#         yield tokens\n",
    "\n",
    "# vocab = build_vocab_from_iterator(yield_tokens(), specials=specials, max_tokens=25000)\n",
    "# vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# # Load GloVe vectors\n",
    "# vectors = GloVe(name='6B', dim=100)\n",
    "# embedding_matrix = torch.zeros(len(vocab), 100, dtype=torch.float32)\n",
    "# for idx, token in enumerate(vocab.get_itos()):\n",
    "#     if token in vectors.stoi:\n",
    "#         embedding_matrix[idx] = vectors[token].float()\n",
    "#     else:\n",
    "#         embedding_matrix[idx] = torch.randn(100) * 0.1\n",
    "\n",
    "# def encode(tokens):\n",
    "#     return [vocab[t] for t in tokens]\n",
    "\n",
    "# df['context_ids'] = df['context_tok'].apply(encode)\n",
    "# df['question_ids'] = df['question_tok'].apply(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T04:01:13.544693Z",
     "iopub.status.busy": "2025-04-22T04:01:13.544438Z",
     "iopub.status.idle": "2025-04-22T04:01:13.554380Z",
     "shell.execute_reply": "2025-04-22T04:01:13.553673Z",
     "shell.execute_reply.started": "2025-04-22T04:01:13.544675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_ids</th>\n",
       "      <th>question_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12947</th>\n",
       "      <td>[2, 377, 135, 4, 1249, 7, 7799, 3, 9, 135, 15,...</td>\n",
       "      <td>[78, 11, 2, 377, 135, 4, 1249, 7, 7799, 172, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12697</th>\n",
       "      <td>[6, 2, 855, 3, 2, 16898, 57, 8, 1131, 2, 669, ...</td>\n",
       "      <td>[13, 99, 2, 120, 1143, 8, 223, 10]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context_ids  \\\n",
       "12947  [2, 377, 135, 4, 1249, 7, 7799, 3, 9, 135, 15,...   \n",
       "12697  [6, 2, 855, 3, 2, 16898, 57, 8, 1131, 2, 669, ...   \n",
       "\n",
       "                                           question_ids  \n",
       "12947  [78, 11, 2, 377, 135, 4, 1249, 7, 7799, 172, 10]  \n",
       "12697                [13, 99, 2, 120, 1143, 8, 223, 10]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df_sorted[['context_idx', 'question_idx', 'answer_idx']].head(2)\n",
    "# df[['context_ids', 'question_ids']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T04:01:17.293030Z",
     "iopub.status.busy": "2025-04-22T04:01:17.292769Z",
     "iopub.status.idle": "2025-04-22T04:01:17.755927Z",
     "shell.execute_reply": "2025-04-22T04:01:17.755140Z",
     "shell.execute_reply.started": "2025-04-22T04:01:17.293013Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>context_tok</th>\n",
       "      <th>question_tok</th>\n",
       "      <th>answer_tok</th>\n",
       "      <th>context_ids</th>\n",
       "      <th>question_ids</th>\n",
       "      <th>start_pos</th>\n",
       "      <th>end_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12947</th>\n",
       "      <td>The Oklahoma School of Science and Mathematics...</td>\n",
       "      <td>Where is The Oklahoma School of Science and Ma...</td>\n",
       "      <td>Oklahoma City</td>\n",
       "      <td>137</td>\n",
       "      <td>[the, oklahoma, school, of, science, and, math...</td>\n",
       "      <td>[where, is, the, oklahoma, school, of, science...</td>\n",
       "      <td>[oklahoma, city]</td>\n",
       "      <td>[2, 377, 135, 4, 1249, 7, 7799, 3, 9, 135, 15,...</td>\n",
       "      <td>[78, 11, 2, 377, 135, 4, 1249, 7, 7799, 172, 10]</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12697</th>\n",
       "      <td>In the past, the Malays used to call the Portu...</td>\n",
       "      <td>What does the term refer to now?</td>\n",
       "      <td>the modern Kristang creoles of Malaysia</td>\n",
       "      <td>111</td>\n",
       "      <td>[in, the, past, ,, the, malays, used, to, call...</td>\n",
       "      <td>[what, does, the, term, refer, to, now, ?]</td>\n",
       "      <td>[the, modern, kristang, creoles, of, malaysia]</td>\n",
       "      <td>[6, 2, 855, 3, 2, 16898, 57, 8, 1131, 2, 669, ...</td>\n",
       "      <td>[13, 99, 2, 120, 1143, 8, 223, 10]</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context  \\\n",
       "12947  The Oklahoma School of Science and Mathematics...   \n",
       "12697  In the past, the Malays used to call the Portu...   \n",
       "\n",
       "                                                question  \\\n",
       "12947  Where is The Oklahoma School of Science and Ma...   \n",
       "12697                  What does the term refer to now?    \n",
       "\n",
       "                                   answer_text  answer_start  \\\n",
       "12947                            Oklahoma City           137   \n",
       "12697  the modern Kristang creoles of Malaysia           111   \n",
       "\n",
       "                                             context_tok  \\\n",
       "12947  [the, oklahoma, school, of, science, and, math...   \n",
       "12697  [in, the, past, ,, the, malays, used, to, call...   \n",
       "\n",
       "                                            question_tok  \\\n",
       "12947  [where, is, the, oklahoma, school, of, science...   \n",
       "12697         [what, does, the, term, refer, to, now, ?]   \n",
       "\n",
       "                                           answer_tok  \\\n",
       "12947                                [oklahoma, city]   \n",
       "12697  [the, modern, kristang, creoles, of, malaysia]   \n",
       "\n",
       "                                             context_ids  \\\n",
       "12947  [2, 377, 135, 4, 1249, 7, 7799, 3, 9, 135, 15,...   \n",
       "12697  [6, 2, 855, 3, 2, 16898, 57, 8, 1131, 2, 669, ...   \n",
       "\n",
       "                                           question_ids  start_pos  end_pos  \n",
       "12947  [78, 11, 2, 377, 135, 4, 1249, 7, 7799, 172, 10]       27.0     28.0  \n",
       "12697                [13, 99, 2, 120, 1143, 8, 223, 10]       23.0     28.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # ## Compute start and end positions\n",
    "\n",
    "# def find_sublist(full, sub):\n",
    "#     for i in range(len(full) - len(sub) + 1):\n",
    "#         if full[i:i+len(sub)] == sub:\n",
    "#             return i\n",
    "#     return -1\n",
    "\n",
    "# start_positions = []\n",
    "# end_positions = []\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     context = row['context_tok']\n",
    "#     answer = row['answer_tok']\n",
    "#     start_idx = find_sublist(context, answer)\n",
    "#     if start_idx == -1:\n",
    "#         start_positions.append(None)\n",
    "#         end_positions.append(None)\n",
    "#     else:\n",
    "#         start_positions.append(start_idx)\n",
    "#         end_positions.append(start_idx + len(answer) - 1)\n",
    "\n",
    "# df['start_pos'] = start_positions\n",
    "# df['end_pos'] = end_positions\n",
    "\n",
    "# df = df[df['start_pos'].notnull()]  # Remove problematic rows\n",
    "\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFdG-NcICVu2"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T17:25:29.441974Z",
     "iopub.status.busy": "2025-04-22T17:25:29.441337Z",
     "iopub.status.idle": "2025-04-22T17:25:29.447285Z",
     "shell.execute_reply": "2025-04-22T17:25:29.446512Z",
     "shell.execute_reply.started": "2025-04-22T17:25:29.441948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QA_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QA_Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.start_linear = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.end_linear = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        start_logits = self.start_linear(sequence_output).squeeze(-1)\n",
    "        end_logits = self.end_linear(sequence_output).squeeze(-1)\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T17:25:33.841471Z",
     "iopub.status.busy": "2025-04-22T17:25:33.841183Z",
     "iopub.status.idle": "2025-04-22T17:25:33.847015Z",
     "shell.execute_reply": "2025-04-22T17:25:33.846241Z",
     "shell.execute_reply.started": "2025-04-22T17:25:33.841451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(row['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(row['attention_mask'], dtype=torch.long),\n",
    "            'start': torch.tensor(row['start_positions'], dtype=torch.long),\n",
    "            'end': torch.tensor(row['end_positions'], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T17:25:37.530400Z",
     "iopub.status.busy": "2025-04-22T17:25:37.530100Z",
     "iopub.status.idle": "2025-04-22T17:25:37.534935Z",
     "shell.execute_reply": "2025-04-22T17:25:37.534335Z",
     "shell.execute_reply.started": "2025-04-22T17:25:37.530379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    start = torch.stack([item['start'] for item in batch])\n",
    "    end = torch.stack([item['end'] for item in batch])\n",
    "    return input_ids, attention_mask, start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T17:25:39.607874Z",
     "iopub.status.busy": "2025-04-22T17:25:39.607595Z",
     "iopub.status.idle": "2025-04-22T17:25:39.612497Z",
     "shell.execute_reply": "2025-04-22T17:25:39.611848Z",
     "shell.execute_reply.started": "2025-04-22T17:25:39.607855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = QADataset(train_encoded)\n",
    "val_dataset = QADataset(val_encoded)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T04:01:31.661319Z",
     "iopub.status.busy": "2025-04-22T04:01:31.660557Z",
     "iopub.status.idle": "2025-04-22T04:01:31.668280Z",
     "shell.execute_reply": "2025-04-22T04:01:31.667550Z",
     "shell.execute_reply.started": "2025-04-22T04:01:31.661291Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745185663180,
     "user": {
      "displayName": "Sherifa Hammoud",
      "userId": "01658453922136634541"
     },
     "user_tz": -120
    },
    "id": "ur23aL6o1L4x",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class QADataset(Dataset):\n",
    "#     def __init__(self, df, max_len=512):\n",
    "#         self.df = df\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.df.iloc[idx]\n",
    "#         context = row['context_ids'][:self.max_len]\n",
    "#         question = row['question_ids'][:64]\n",
    "#         return {\n",
    "#             'context': torch.tensor(context),\n",
    "#             'question': torch.tensor(question),\n",
    "#             'start': torch.tensor(row['start_pos']),\n",
    "#             'end': torch.tensor(row['end_pos']),\n",
    "#         }\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     def pad(seq, max_len):\n",
    "#         return F.pad(seq, (0, max_len - len(seq)), value=vocab['<pad>'])\n",
    "\n",
    "#     context_lens = [len(x['context']) for x in batch]\n",
    "#     question_lens = [len(x['question']) for x in batch]\n",
    "\n",
    "#     context_max = max(context_lens)\n",
    "#     question_max = max(question_lens)\n",
    "\n",
    "#     context = torch.stack([pad(x['context'], context_max) for x in batch])\n",
    "#     question = torch.stack([pad(x['question'], question_max) for x in batch])\n",
    "#     start = torch.stack([x['start'] for x in batch])\n",
    "#     end = torch.stack([x['end'] for x in batch])\n",
    "\n",
    "#     return context, question, start, end\n",
    "\n",
    "# # train_dataset = QADataset(df)\n",
    "# # train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOW-6Mt9CYkw"
   },
   "source": [
    "### Model (Encoder + Attention + Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T04:01:39.107935Z",
     "iopub.status.busy": "2025-04-22T04:01:39.107665Z",
     "iopub.status.idle": "2025-04-22T04:01:39.114771Z",
     "shell.execute_reply": "2025-04-22T04:01:39.114105Z",
     "shell.execute_reply.started": "2025-04-22T04:01:39.107914Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745185663188,
     "user": {
      "displayName": "Sherifa Hammoud",
      "userId": "01658453922136634541"
     },
     "user_tz": -120
    },
    "id": "_V_HBu7o1ORU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class EncoderQA(nn.Module):\n",
    "#     def __init__(self, vocab_size, emb_dim, hidden_dim, embedding_matrix, dropout=0.3):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "#         self.context_lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "#         self.question_lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "\n",
    "#         self.linear_start = nn.Linear(hidden_dim * 2, 1)\n",
    "#         self.linear_end = nn.Linear(hidden_dim * 2, 1)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, context, question):\n",
    "#         context_embed = self.embedding(context)\n",
    "#         question_embed = self.embedding(question)\n",
    "\n",
    "#         context_out, _ = self.context_lstm(context_embed)\n",
    "#         question_out, _ = self.question_lstm(question_embed)\n",
    "\n",
    "#         # Attention mechanism: Compute dot product attention\n",
    "#         attention_scores = torch.matmul(context_out, question_out.transpose(1, 2))\n",
    "#         attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "#         attended_context = torch.matmul(attention_weights, question_out)\n",
    "\n",
    "#         # Apply dropout to regularize the model\n",
    "#         attended_context = self.dropout(attended_context)\n",
    "\n",
    "#         start_logits = self.linear_start(attended_context).squeeze(-1)\n",
    "#         end_logits = self.linear_end(attended_context).squeeze(-1)\n",
    "\n",
    "#         return start_logits.float(), end_logits.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T17:27:37.712795Z",
     "iopub.status.busy": "2025-04-22T17:27:37.712428Z",
     "iopub.status.idle": "2025-04-22T17:27:37.729973Z",
     "shell.execute_reply": "2025-04-22T17:27:37.729306Z",
     "shell.execute_reply.started": "2025-04-22T17:27:37.712772Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12947</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12697</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12696</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24533</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24531</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74447</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62008</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28774</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28775</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84709</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "12947  [input_ids, token_type_ids, attention_mask, of...\n",
       "12697  [input_ids, token_type_ids, attention_mask, of...\n",
       "12696  [input_ids, token_type_ids, attention_mask, of...\n",
       "24533  [input_ids, token_type_ids, attention_mask, of...\n",
       "24531  [input_ids, token_type_ids, attention_mask, of...\n",
       "...                                                  ...\n",
       "74447  [input_ids, token_type_ids, attention_mask, of...\n",
       "62008  [input_ids, token_type_ids, attention_mask, of...\n",
       "28774  [input_ids, token_type_ids, attention_mask, of...\n",
       "28775  [input_ids, token_type_ids, attention_mask, of...\n",
       "84709  [input_ids, token_type_ids, attention_mask, of...\n",
       "\n",
       "[16000 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAD71_ayCcDj"
   },
   "source": [
    "### Training Setup & Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T12:54:00.265523Z",
     "iopub.status.busy": "2025-04-22T12:54:00.264829Z",
     "iopub.status.idle": "2025-04-22T12:54:01.994226Z",
     "shell.execute_reply": "2025-04-22T12:54:01.993182Z",
     "shell.execute_reply.started": "2025-04-22T12:54:00.265497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                              | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/3941875818.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# print(f'train_loader: ${train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;31m# Plot training and validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/3941875818.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, loss_fn, num_epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Step 2: Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/2155550949.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstart_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m   1081\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = QA_Model().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def train_model(model, train_loader, optimizer, loss_fn, num_epochs=3):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        epoch_start_time = time.time()  # Track the time for the epoch\n",
    "\n",
    "        # Use tqdm for a progress bar\n",
    "        for i, (input_ids, attention_mask, start, end) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\", ncols=100)):\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Step 1: Move to GPU\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            start = start.to(device)\n",
    "            end = end.to(device)\n",
    "            t1 = time.time()\n",
    "        \n",
    "            # Step 2: Forward pass\n",
    "            start_logits, end_logits = model(input_ids, attention_mask)\n",
    "            t2 = time.time()\n",
    "        \n",
    "            # Step 3: Loss and Backward\n",
    "            loss = loss_fn(start_logits, start) + loss_fn(end_logits, end)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            t3 = time.time()\n",
    "        \n",
    "            # Step 4: Optimizer step\n",
    "            optimizer.step()\n",
    "            t4 = time.time()\n",
    "\n",
    "            if i < 10:\n",
    "                print(f\"Batch {i}: To GPU: {t1 - t0:.3f}s | Forward: {t2 - t1:.3f}s | Backward: {t3 - t2:.3f}s | Step: {t4 - t3:.3f}s\")\n",
    "            \n",
    "            # if i == 10: break\n",
    "                \n",
    "            # input_ids = input_ids.to(device)\n",
    "            # attention_mask = attention_mask.to(device)\n",
    "            # start = start.to(device)\n",
    "            # end = end.to(device)\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            # start_logits, end_logits = model(input_ids, attention_mask)\n",
    "            # loss = loss_fn(start_logits, start) + loss_fn(end_logits, end)\n",
    "\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "\n",
    "            # total_train_loss += loss.item()\n",
    "\n",
    "            # Print every 100 steps in the training loop\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Step {i + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Calculate validation loss\n",
    "        val_loss = evaluate_model(model, val_loader, loss_fn)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# print(f'train_loader: ${train')\n",
    "train_losses, val_losses = train_model(model, train_loader, optimizer, loss_fn, num_epochs=20)\n",
    "# Plot training and validation loss\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T12:43:05.327188Z",
     "iopub.status.busy": "2025-04-22T12:43:05.326903Z",
     "iopub.status.idle": "2025-04-22T12:44:06.688924Z",
     "shell.execute_reply": "2025-04-22T12:44:06.688055Z",
     "shell.execute_reply.started": "2025-04-22T12:43:05.327169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.6121\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/3136328187.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation Loss: {eval_losses:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Plot training and validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, val_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, start, end = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            start = start.to(device)\n",
    "            end = end.to(device)\n",
    "            t1 = time.time()\n",
    "            start_logits, end_logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(start_logits, start) + loss_fn(end_logits, end)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "eval_losses = evaluate_model(model, val_loader, loss_fn)\n",
    "print(f\"Validation Loss: {eval_losses:.4f}\")\n",
    "# Plot evaluation loss\n",
    "plt.plot(eval_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def test_model(model, dataset, tokenizer, max_samples=10):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(max_samples, len(dataset))):\n",
    "            sample = dataset[i]\n",
    "            input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "            attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "            start_true = sample['start'].item()\n",
    "            end_true = sample['end'].item()\n",
    "\n",
    "            # Get model predictions\n",
    "            start_logits, end_logits = model(input_ids, attention_mask)\n",
    "            pred_start = torch.argmax(start_logits, dim=1).item()\n",
    "            pred_end = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "            # Clip to valid range\n",
    "            if pred_start > pred_end:\n",
    "                pred_start, pred_end = pred_end, pred_start\n",
    "\n",
    "            # Decode context, predicted answer, and true answer\n",
    "            context_tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'].tolist())\n",
    "            pred_answer = context_tokens[pred_start:pred_end+1]\n",
    "            true_answer = context_tokens[start_true:end_true+1]\n",
    "\n",
    "            pred_answer_text = tokenizer.convert_tokens_to_string(pred_answer)\n",
    "            true_answer_text = tokenizer.convert_tokens_to_string(true_answer)\n",
    "            context_text = tokenizer.convert_tokens_to_string(context_tokens)\n",
    "            question_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "            # Calculate similarity metrics\n",
    "            jaccard = jaccard_score(\n",
    "                set(pred_answer_text.split()), \n",
    "                set(true_answer_text.split()), \n",
    "                average='macro'\n",
    "            )\n",
    "            bleu = sentence_bleu([true_answer_text.split()], pred_answer_text.split())\n",
    "\n",
    "            # Print details\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(f\"Context: {context_text}\")\n",
    "            print(f\"Question: {question_text}\")\n",
    "            print(f\"Predicted Answer: {pred_answer_text}\")\n",
    "            print(f\"True Answer: {true_answer_text}\")\n",
    "            print(f\"Jaccard Similarity: {jaccard:.4f}\")\n",
    "            print(f\"BLEU Score: {bleu:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            results.append({\n",
    "                \"context\": context_text,\n",
    "                \"question\": question_text,\n",
    "                \"predicted_answer\": pred_answer_text,\n",
    "                \"true_answer\": true_answer_text,\n",
    "                \"jaccard\": jaccard,\n",
    "                \"bleu\": bleu\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the testing process\n",
    "test_results = test_model(model, val_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T04:05:55.714583Z",
     "iopub.status.busy": "2025-04-22T04:05:55.714208Z",
     "iopub.status.idle": "2025-04-22T04:06:29.746804Z",
     "shell.execute_reply": "2025-04-22T04:06:29.745806Z",
     "shell.execute_reply.started": "2025-04-22T04:05:55.714561Z"
    },
    "executionInfo": {
     "elapsed": 2085303,
     "status": "ok",
     "timestamp": 1745188578504,
     "user": {
      "displayName": "Sherifa Hammoud",
      "userId": "01658453922136634541"
     },
     "user_tz": -120
    },
    "id": "VXooMzEc1QmL",
    "outputId": "4e5f982a-d5f0-423b-a0cc-4ca6413dcd0d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 8.2623, Validation Loss: 7.2884\n",
      "Epoch 2, Training Loss: 7.0736, Validation Loss: 6.9463\n",
      "Epoch 3, Training Loss: 6.7766, Validation Loss: 6.8104\n",
      "Epoch 4, Training Loss: 6.6115, Validation Loss: 6.7409\n",
      "Epoch 5, Training Loss: 6.4355, Validation Loss: 6.6933\n",
      "Epoch 6, Training Loss: 6.2452, Validation Loss: 6.6355\n",
      "Epoch 7, Training Loss: 6.0362, Validation Loss: 6.6488\n",
      "Epoch 8, Training Loss: 5.8400, Validation Loss: 6.6328\n",
      "Epoch 9, Training Loss: 5.6340, Validation Loss: 6.5947\n",
      "Epoch 10, Training Loss: 5.4059, Validation Loss: 6.5028\n",
      "Epoch 11, Training Loss: 5.2022, Validation Loss: 6.6447\n",
      "Epoch 12, Training Loss: 5.0028, Validation Loss: 6.7459\n",
      "Epoch 13, Training Loss: 4.8243, Validation Loss: 6.7187\n",
      "Early stopping triggered at epoch 13.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_108/182034964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Evaluate the model on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nValidation Evaluation -> Exact Match: {results['EM']:.4f}, F1 Score: {results['F1']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_108/2563382808.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataset, tokenizer, vocab, max_samples)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mcontext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mpred_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpred_end\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtrue_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_true\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_true\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mpred_answer_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" ##\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "# # Split dataset into train and validation (80% training, 20% validation)\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create Dataset objects for training and validation\n",
    "# train_dataset = QADataset(train_df)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# val_dataset = QADataset(val_df)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# # Function to compute loss\n",
    "# def compute_loss(model, loader, loss_fn):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for context, question, start, end in loader:\n",
    "#             context = context.to(device)\n",
    "#             question = question.to(device)\n",
    "#             start = start.to(device)\n",
    "#             end = end.to(device)\n",
    "\n",
    "#             start_logits, end_logits = model(context, question)\n",
    "\n",
    "#             start_logits = start_logits.float()\n",
    "#             end_logits = end_logits.float()\n",
    "            \n",
    "#             start = start.long()\n",
    "#             end = end.long() \n",
    "            \n",
    "#             loss = loss_fn(start_logits, start) + loss_fn(end_logits, end)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / len(loader)\n",
    "\n",
    "# # Early stopping parameters\n",
    "# best_val_loss = float('inf')\n",
    "# patience = 3  # How many epochs to wait for improvement\n",
    "# epochs_no_improve = 0  # Counter for early stopping\n",
    "\n",
    "# # Training loop with validation loss monitoring\n",
    "# model = EncoderQA(len(vocab), 100, 128, embedding_matrix).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(30):  # Number of epochs you want to train\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for context, question, start, end in train_loader:\n",
    "#         context = context.to(device)\n",
    "#         question = question.to(device)\n",
    "#         start = start.to(device)\n",
    "#         end = end.to(device)\n",
    "\n",
    "#         start_logits, end_logits = model(context, question)\n",
    "#         start_logits = start_logits.float()\n",
    "#         end_logits = end_logits.float()\n",
    "        \n",
    "#         start = start.long()\n",
    "#         end = end.long()\n",
    "        \n",
    "#         loss = loss_fn(start_logits, start) + loss_fn(end_logits, end)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     # Calculate validation loss\n",
    "#     val_loss = compute_loss(model, val_loader, loss_fn)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}, Training Loss: {total_loss / len(train_loader):.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "#     # Early stopping check\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         epochs_no_improve = 0  # Reset the counter\n",
    "#     else:\n",
    "#         epochs_no_improve += 1\n",
    "\n",
    "#     # Stop training if validation loss hasn't improved for 'patience' epochs\n",
    "#     if epochs_no_improve >= patience:\n",
    "#         print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "#         break\n",
    "\n",
    "# # Evaluate the model on the validation set\n",
    "# results = evaluate(model, val_dataset, decode, vocab)\n",
    "# print(f\"\\nValidation Evaluation -> Exact Match: {results['EM']:.4f}, F1 Score: {results['F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T04:09:55.042797Z",
     "iopub.status.busy": "2025-04-22T04:09:55.042504Z",
     "iopub.status.idle": "2025-04-22T04:10:11.944557Z",
     "shell.execute_reply": "2025-04-22T04:10:11.943736Z",
     "shell.execute_reply.started": "2025-04-22T04:09:55.042776Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation -> Exact Match: 0.0600, F1 Score: 0.1196\n"
     ]
    }
   ],
   "source": [
    "# ## Evaluation\n",
    "\n",
    "def decode(tokens):\n",
    "    return [vocab.get_itos()[token] for token in tokens]\n",
    "\n",
    "def evaluate(model, dataset, tokenizer, vocab, max_samples=100):\n",
    "    model.eval()\n",
    "    EM_total, F1_total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(max_samples, len(dataset))):\n",
    "            sample = dataset[i]\n",
    "            context = sample['context'].unsqueeze(0).to(device)\n",
    "            question = sample['question'].unsqueeze(0).to(device)\n",
    "            start_true = sample['start'].item()\n",
    "            end_true = sample['end'].item()\n",
    "\n",
    "            start_true = int(start_true)\n",
    "            end_true = int(end_true)\n",
    "\n",
    "            start_logits, end_logits = model(context, question)\n",
    "            pred_start = torch.argmax(start_logits, dim=1).item()\n",
    "            pred_end = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "            # Clip to valid range\n",
    "            if pred_start > pred_end:\n",
    "                pred_start, pred_end = pred_end, pred_start\n",
    "\n",
    "            context_tokens = decode(sample['context'])\n",
    "            pred_answer = context_tokens[pred_start:pred_end+1]\n",
    "            true_answer = context_tokens[start_true:end_true+1]\n",
    "\n",
    "            pred_answer_text = \" \".join(pred_answer).replace(\" ##\", \"\")\n",
    "            true_answer_text = \" \".join(true_answer).replace(\" ##\", \"\")\n",
    "\n",
    "            # Exact Match\n",
    "            EM = int(pred_answer_text == true_answer_text)\n",
    "            EM_total += EM\n",
    "\n",
    "            # F1 Score\n",
    "            pred_set = set(pred_answer_text.split())\n",
    "            true_set = set(true_answer_text.split())\n",
    "            common = pred_set & true_set\n",
    "            if len(common) == 0:\n",
    "                F1 = 0\n",
    "            else:\n",
    "                precision = len(common) / len(pred_set)\n",
    "                recall = len(common) / len(true_set)\n",
    "                F1 = 2 * precision * recall / (precision + recall)\n",
    "            F1_total += F1\n",
    "\n",
    "    return {\n",
    "        \"EM\": EM_total / max_samples,\n",
    "        \"F1\": F1_total / max_samples\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate(model, val_dataset, decode, vocab)\n",
    "print(f\"\\nEvaluation -> Exact Match: {results['EM']:.4f}, F1 Score: {results['F1']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPEwqx3mKZah2dRxpKoErKN",
   "collapsed_sections": [
    "adtfwaAwCPq0",
    "BNL7-BZQCIJM",
    "62xRU1SMCBBk",
    "zFdG-NcICVu2",
    "qOW-6Mt9CYkw"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
