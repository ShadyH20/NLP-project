{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShadyH20/NLP-project/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: farasa in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (0.0.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: farasapy in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (0.0.14)\n",
            "Requirement already satisfied: requests in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from farasapy) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from farasapy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from requests->farasapy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from requests->farasapy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from requests->farasapy) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from requests->farasapy) (2024.2.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: arabic-reshaper in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (3.0.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: python-bidi in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (0.6.6)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: qalsadi in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (0.5)\n",
            "Requirement already satisfied: Arabic-Stopwords>=0.4.2 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from qalsadi) (0.4.3)\n",
            "Requirement already satisfied: alyahmor>=0.2 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from qalsadi) (0.2)\n",
            "Requirement already satisfied: arramooz-pysqlite>=0.4.2 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from qalsadi) (0.4.2)\n",
            "Requirement already satisfied: codernitydb3 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from qalsadi) (0.6.0)\n",
            "Requirement already satisfied: libqutrub>=1.2.3 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from qalsadi) (1.2.4.1)\n",
            "Requirement already satisfied: mysam-tagmanager>=0.3.3 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from qalsadi) (0.4)\n",
            "Requirement already satisfied: naftawayh>=0.3 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from qalsadi) (0.4)\n",
            "Requirement already satisfied: pickledb>=0.9.2 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from qalsadi) (1.3.2)\n",
            "Requirement already satisfied: pyarabic>=0.6.7 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from qalsadi) (0.6.15)\n",
            "Requirement already satisfied: tashaphyne>=0.3.4.1 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from qalsadi) (0.3.6)\n",
            "Requirement already satisfied: orjson in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from pickledb>=0.9.2->qalsadi) (3.10.15)\n",
            "Requirement already satisfied: aiofiles in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from pickledb>=0.9.2->qalsadi) (24.1.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from pyarabic>=0.6.7->qalsadi) (1.15.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langdetect in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (1.0.9)\n",
            "Requirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from langdetect) (1.15.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: googletrans in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (4.0.2)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from httpx[http2]>=0.27.2->googletrans) (0.28.1)\n",
            "Requirement already satisfied: anyio in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.3.0)\n",
            "Requirement already satisfied: certifi in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.0.4)\n",
            "Requirement already satisfied: idna in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from httpx[http2]>=0.27.2->googletrans) (4.2.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.12.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install farasa\n",
        "!pip install farasapy\n",
        "!pip install arabic-reshaper\n",
        "!pip install python-bidi\n",
        "!pip install qalsadi\n",
        "!pip install langdetect\n",
        "!pip install googletrans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sherifahammoud/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sherifahammoud/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sherifahammoud/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/sherifahammoud/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /Users/sherifahammoud/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sherifahammoud/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/sherifahammoud/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 204,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from itertools import islice\n",
        "from nltk.util import ngrams\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from farasa.segmenter import FarasaSegmenter\n",
        "from farasa.stemmer import FarasaStemmer\n",
        "from qalsadi.lemmatizer import Lemmatizer\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "from tashaphyne.stemming import ArabicLightStemmer\n",
        "from farasa.diacratizer import FarasaDiacritizer\n",
        "from qalsadi.lemmatizer import Lemmatizer\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from nltk import tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unlabeled Data (Da7ee7, Elsaha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "8BS8I-FnoSI0"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = \"./YoutubeChannels\"\n",
        "CHANNELS = [\"Da7ee7\", \"Elsaha\"]\n",
        "data = []\n",
        "\n",
        "timestamp_pattern = re.compile(r\"^\\d+\\.\\d+:\\s*\")  # Removes timestamps at start of lines\n",
        "\n",
        "for channel in CHANNELS:\n",
        "    raw_data_path = os.path.join(BASE_DIR, channel, \"Raw Data\")\n",
        "    \n",
        "    if os.path.exists(raw_data_path):\n",
        "        for file_name in os.listdir(raw_data_path):\n",
        "            if file_name.endswith(\".txt\"):\n",
        "                file_path = os.path.join(raw_data_path, file_name)\n",
        "                \n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    raw_lines = f.readlines()\n",
        "                \n",
        "                # Remove timestamps and extra whitespace from lines\n",
        "                cleaned_text = \"\\n\".join(re.sub(timestamp_pattern, \"\", line).strip() for line in raw_lines)\n",
        "                episode_title = file_name.replace(\".txt\", \"\").strip()\n",
        "                \n",
        "                # Append the entire script as one row\n",
        "                data.append([channel, episode_title, cleaned_text, \"\", \"\"])\n",
        "\n",
        "# Create DataFrame\n",
        "unlabelled_df = pd.DataFrame(data, columns=[\"Channel\", \"EpisodeTitle\", \"Original Script\", \"Processed Script\", \"Category\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Labeled Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = \"./YoutubeChannels\"\n",
        "LABELLED_CHANNELS_1 = [\"Kefaya Ba2a\", \"Business Bel Araby\", \"B Hodoo2\"]\n",
        "LABELLED_CHANNELS_2 = [\"Eqtisad_Al_Hadaraa\", \"Fi_Al_Hadaraa\", \"Al_Mokhbir_Al_Eqtisadi\"]\n",
        "LABELLED_CHANNELS_3 = [\"Akhdar\"]\n",
        "data = []\n",
        "\n",
        "def process_labelled_channel_1(channel):\n",
        "    raw_data_path = os.path.join(BASE_DIR, channel, \"raw\")\n",
        "    annotations_path = os.path.join(BASE_DIR, channel, \"annotations.json\")\n",
        "    \n",
        "    if os.path.exists(annotations_path):\n",
        "        with open(annotations_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            annotations = json.load(f)\n",
        "        \n",
        "        metadata_map = {\n",
        "            item[\"title\"].strip()[:10] if channel == \"B Hodoo2\" else item[\"title\"].strip(): item\n",
        "            for item in annotations\n",
        "        }\n",
        "        \n",
        "        if os.path.exists(raw_data_path):\n",
        "            for file_name in os.listdir(raw_data_path):\n",
        "                if file_name.endswith(\".txt\"):\n",
        "                    file_path = os.path.join(raw_data_path, file_name)\n",
        "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                        raw_text = f.read().strip()\n",
        "                    \n",
        "                    episode_title = file_name.replace(\".txt\", \"\").strip()\n",
        "                    match_key = episode_title[:10] if channel == \"B Hodoo2\" else episode_title\n",
        "                    metadata = metadata_map.get(match_key, {})\n",
        "                    \n",
        "                    data.append([\n",
        "                        channel,\n",
        "                        episode_title,\n",
        "                        raw_text,\n",
        "                        \"\",\n",
        "                        metadata.get(\"dialogue\", \"\"),\n",
        "                        metadata.get(\"type\", \"\"),\n",
        "                        metadata.get(\"length\", \"\"),\n",
        "                        metadata.get(\"category\", \"\")\n",
        "                    ])\n",
        "\n",
        "def process_labelled_channel_2(channel):\n",
        "    raw_data_path = os.path.join(BASE_DIR, channel, \"raw_data\")\n",
        "    metadata_path = os.path.join(BASE_DIR, channel, \"metadata\")\n",
        "    \n",
        "    if os.path.exists(raw_data_path) and os.path.exists(metadata_path):\n",
        "        for file_name in os.listdir(raw_data_path):\n",
        "            if file_name.endswith(\".txt\"):\n",
        "                episode_title = file_name.replace(\".txt\", \"\").strip()\n",
        "                raw_file_path = os.path.join(raw_data_path, file_name)\n",
        "                metadata_file_path = os.path.join(metadata_path, episode_title + \".json\")\n",
        "                \n",
        "                with open(raw_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    raw_text = f.read().strip()\n",
        "                \n",
        "                if os.path.exists(metadata_file_path):\n",
        "                    with open(metadata_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                        metadata = json.load(f)\n",
        "                    category = metadata.get(\"categories\", [\"\"])[0]\n",
        "                    \n",
        "                    data.append([\n",
        "                        channel,\n",
        "                        episode_title,\n",
        "                        raw_text,\n",
        "                        \"\",\n",
        "                        metadata.get(\"dialogue\", \"\"),\n",
        "                        metadata.get(\"type\", \"\"),\n",
        "                        metadata.get(\"length\", \"\"),\n",
        "                        category\n",
        "                    ])\n",
        "\n",
        "def find_akhdar_raw_and_metadata(path):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        if \"raw_data\" in dirs and \"metadata\" in dirs:\n",
        "            process_labelled_channel_2(root)\n",
        "\n",
        "def process_labelled_channel_3(channel):\n",
        "    base_path = os.path.join(BASE_DIR, channel)\n",
        "    find_akhdar_raw_and_metadata(base_path)\n",
        "\n",
        "# Process all channels\n",
        "for channel in LABELLED_CHANNELS_1:\n",
        "    process_labelled_channel_1(channel)\n",
        "\n",
        "for channel in LABELLED_CHANNELS_2:\n",
        "    process_labelled_channel_2(channel)\n",
        "\n",
        "for channel in LABELLED_CHANNELS_3:\n",
        "    process_labelled_channel_3(channel)\n",
        "\n",
        "labelled_df = pd.DataFrame(data, columns=[\"Channel\", \"Episode Title\", \"Original Script\", \"Processed Script\", \"Dialogue\", \"Type\", \"Length\", \"Category\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📍 Checking Unlabelled DataFrame:\n",
            "📌 Sample rows from channel: Da7ee7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>EpisodeTitle</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>Da7ee7</td>\n",
              "      <td>نابليون في القاهرة  الدحيح</td>\n",
              "      <td>يا بوي! دا أني تعبت جوي!\\n!Bonjour\\nأفندم؟!\\nP...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Da7ee7</td>\n",
              "      <td>هانيبال  الدحيح</td>\n",
              "      <td>أوه، \"ماركينوس\"، لم أعُد أستطيع النوم،\\nمن فرط...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Da7ee7</td>\n",
              "      <td>تاريخ التشريح  الدحيح</td>\n",
              "      <td>أنا الدكتور \"ياسر الطائي\"،\\nأكتب الآن تقرير تش...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Channel                EpisodeTitle  \\\n",
              "54  Da7ee7  نابليون في القاهرة  الدحيح   \n",
              "6   Da7ee7             هانيبال  الدحيح   \n",
              "1   Da7ee7       تاريخ التشريح  الدحيح   \n",
              "\n",
              "                                      Original Script Processed Script  \\\n",
              "54  يا بوي! دا أني تعبت جوي!\\n!Bonjour\\nأفندم؟!\\nP...                    \n",
              "6   أوه، \"ماركينوس\"، لم أعُد أستطيع النوم،\\nمن فرط...                    \n",
              "1   أنا الدكتور \"ياسر الطائي\"،\\nأكتب الآن تقرير تش...                    \n",
              "\n",
              "   Category  \n",
              "54           \n",
              "6            \n",
              "1            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "📌 Sample rows from channel: Elsaha\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>EpisodeTitle</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Elsaha</td>\n",
              "      <td>أقوى من الخوف والإعاقة.. حكاية عبد الرحمن صلاح</td>\n",
              "      <td>[موسيقى]\\nاول ما لبست الطرف طبعا كان مضايقني ب...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>Elsaha</td>\n",
              "      <td>ليست دينية مسيحية.. حكاية جريدة وطني</td>\n",
              "      <td>وطني صدرت\\n1958 وانا كان سني ثمان سنين والدي ا...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>Elsaha</td>\n",
              "      <td>واتر البحري.. مصور حياة طبيعية</td>\n",
              "      <td>اي مصور حياه بريه يقول لك لو انت رايح\\nتصور طي...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Channel                                    EpisodeTitle  \\\n",
              "163  Elsaha  أقوى من الخوف والإعاقة.. حكاية عبد الرحمن صلاح   \n",
              "159  Elsaha            ليست دينية مسيحية.. حكاية جريدة وطني   \n",
              "154  Elsaha                  واتر البحري.. مصور حياة طبيعية   \n",
              "\n",
              "                                       Original Script Processed Script  \\\n",
              "163  [موسيقى]\\nاول ما لبست الطرف طبعا كان مضايقني ب...                    \n",
              "159  وطني صدرت\\n1958 وانا كان سني ثمان سنين والدي ا...                    \n",
              "154  اي مصور حياه بريه يقول لك لو انت رايح\\nتصور طي...                    \n",
              "\n",
              "    Category  \n",
              "163           \n",
              "159           \n",
              "154           "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "\n",
            "📍 Checking Labelled DataFrame:\n",
            "📌 Sample rows from channel: Kefaya Ba2a\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Dialogue</th>\n",
              "      <th>Type</th>\n",
              "      <th>Length</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Kefaya Ba2a</td>\n",
              "      <td>بودكاست كفاية بقى “بالله عليك” - العسف الرمضاني</td>\n",
              "      <td>اهلا بكم في بودكاست كفايه بقى النسخه\\nالرمضاني...</td>\n",
              "      <td></td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kefaya Ba2a</td>\n",
              "      <td>بودكاست كفاية بقى - تشيز كيك بدون تشيز</td>\n",
              "      <td>اهلا بكم في بودكاست كفايه بقى البودكاست\\nده يا...</td>\n",
              "      <td></td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Kefaya Ba2a</td>\n",
              "      <td>بودكاست كفاية بقى - كاتش عليه ١</td>\n",
              "      <td>اهلا بكم في بودكاست كفايه بقى البودكاست\\nده يا...</td>\n",
              "      <td></td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Channel                                    Episode Title  \\\n",
              "16  Kefaya Ba2a  بودكاست كفاية بقى “بالله عليك” - العسف الرمضاني   \n",
              "2   Kefaya Ba2a           بودكاست كفاية بقى - تشيز كيك بدون تشيز   \n",
              "5   Kefaya Ba2a                  بودكاست كفاية بقى - كاتش عليه ١   \n",
              "\n",
              "                                      Original Script Processed Script  \\\n",
              "16  اهلا بكم في بودكاست كفايه بقى النسخه\\nالرمضاني...                    \n",
              "2   اهلا بكم في بودكاست كفايه بقى البودكاست\\nده يا...                    \n",
              "5   اهلا بكم في بودكاست كفايه بقى البودكاست\\nده يا...                    \n",
              "\n",
              "   Dialogue     Type Length        Category  \n",
              "16    false  Podcast         People & Blogs  \n",
              "2     false  Podcast         People & Blogs  \n",
              "5     false  Podcast         People & Blogs  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "📌 Sample rows from channel: Business Bel Araby\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Dialogue</th>\n",
              "      <th>Type</th>\n",
              "      <th>Length</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Business Bel Araby</td>\n",
              "      <td>ما معنى النجاح #2</td>\n",
              "      <td>ايه معنى النجاح بالنسبه لك في الحياه\\nعامه عار...</td>\n",
              "      <td></td>\n",
              "      <td>true</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>Education</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Business Bel Araby</td>\n",
              "      <td>من طيار الي رائد اعمال قصة محمد عاصي مؤسس بران...</td>\n",
              "      <td>يعني اول حاجه لازم يكون في فكره ليه عشان\\nانت ...</td>\n",
              "      <td></td>\n",
              "      <td>true</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>Education</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Business Bel Araby</td>\n",
              "      <td>من مهندس الى رئيس مجلس ادارة لشركة مقاولات - ط...</td>\n",
              "      <td>اوحش حاجه تعملها لنفسك ان انت وانت نازل\\nالشغل...</td>\n",
              "      <td></td>\n",
              "      <td>true</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Channel                                      Episode Title  \\\n",
              "22  Business Bel Araby                                  ما معنى النجاح #2   \n",
              "25  Business Bel Araby  من طيار الي رائد اعمال قصة محمد عاصي مؤسس بران...   \n",
              "23  Business Bel Araby  من مهندس الى رئيس مجلس ادارة لشركة مقاولات - ط...   \n",
              "\n",
              "                                      Original Script Processed Script  \\\n",
              "22  ايه معنى النجاح بالنسبه لك في الحياه\\nعامه عار...                    \n",
              "25  يعني اول حاجه لازم يكون في فكره ليه عشان\\nانت ...                    \n",
              "23  اوحش حاجه تعملها لنفسك ان انت وانت نازل\\nالشغل...                    \n",
              "\n",
              "   Dialogue     Type Length        Category  \n",
              "22     true  Podcast              Education  \n",
              "25     true  Podcast              Education  \n",
              "23     true  Podcast         People & Blogs  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "📌 Sample rows from channel: B Hodoo2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Dialogue</th>\n",
              "      <th>Type</th>\n",
              "      <th>Length</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>B Hodoo2</td>\n",
              "      <td>زدت ٢٠ كيلو بسبب الحسد _ بودكاست بهدوء مع كريم...</td>\n",
              "      <td>من حوالي سبع سنين واحده كتبت لي في\\nالتعليقات ...</td>\n",
              "      <td></td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>Education</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>B Hodoo2</td>\n",
              "      <td>٩ نصائح للصحة النفسية غيرت حياتي - الجزء الأول...</td>\n",
              "      <td>السلام عليكم ورحمه الله وبركاته دي نصائح\\nالتل...</td>\n",
              "      <td></td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>Education</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>B Hodoo2</td>\n",
              "      <td>تخلص من الكلام السلبي والتنمر! _ بودكاست بهدوء...</td>\n",
              "      <td>يدخل البيت فبيقول لها\\nوصفي لي الحرامي بوليس ب...</td>\n",
              "      <td></td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>Education</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Channel                                      Episode Title  \\\n",
              "39  B Hodoo2  زدت ٢٠ كيلو بسبب الحسد _ بودكاست بهدوء مع كريم...   \n",
              "41  B Hodoo2  ٩ نصائح للصحة النفسية غيرت حياتي - الجزء الأول...   \n",
              "47  B Hodoo2  تخلص من الكلام السلبي والتنمر! _ بودكاست بهدوء...   \n",
              "\n",
              "                                      Original Script Processed Script  \\\n",
              "39  من حوالي سبع سنين واحده كتبت لي في\\nالتعليقات ...                    \n",
              "41  السلام عليكم ورحمه الله وبركاته دي نصائح\\nالتل...                    \n",
              "47  يدخل البيت فبيقول لها\\nوصفي لي الحرامي بوليس ب...                    \n",
              "\n",
              "   Dialogue     Type Length   Category  \n",
              "39    false  Podcast         Education  \n",
              "41    false  Podcast         Education  \n",
              "47    false  Podcast         Education  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "📌 Sample rows from channel: Fi_Al_Hadaraa\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Dialogue</th>\n",
              "      <th>Type</th>\n",
              "      <th>Length</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>Fi_Al_Hadaraa</td>\n",
              "      <td>هالاند_تريبل_كابتن__في_الحضارة</td>\n",
              "      <td>يعني خلاص؟\\nما فيش منّه تاني؟\\nما خلاص بقى يا ...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:24:18</td>\n",
              "      <td>Entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>Fi_Al_Hadaraa</td>\n",
              "      <td>لماذا_نحتاج_التعلم_بشكل_مستمر_للحفاظ_على_الوظيفة</td>\n",
              "      <td>دلوقتي المؤسسات والشركات بقت مخليها\\nمساله تدر...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:00:58</td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>Fi_Al_Hadaraa</td>\n",
              "      <td>اللغة__كيف_فقدنا_الحساسية_للغة_الجميلة__في_الح...</td>\n",
              "      <td>يا نجف بنّور، صديقي الإنسان. صديقي الإنسان!\\nا...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:17:23</td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Channel                                      Episode Title  \\\n",
              "94  Fi_Al_Hadaraa                     هالاند_تريبل_كابتن__في_الحضارة   \n",
              "85  Fi_Al_Hadaraa   لماذا_نحتاج_التعلم_بشكل_مستمر_للحفاظ_على_الوظيفة   \n",
              "73  Fi_Al_Hadaraa  اللغة__كيف_فقدنا_الحساسية_للغة_الجميلة__في_الح...   \n",
              "\n",
              "                                      Original Script Processed Script  \\\n",
              "94  يعني خلاص؟\\nما فيش منّه تاني؟\\nما خلاص بقى يا ...                    \n",
              "85  دلوقتي المؤسسات والشركات بقت مخليها\\nمساله تدر...                    \n",
              "73  يا نجف بنّور، صديقي الإنسان. صديقي الإنسان!\\nا...                    \n",
              "\n",
              "   Dialogue     Type    Length        Category  \n",
              "94           Youtube  00:24:18   Entertainment  \n",
              "85           Youtube  00:00:58  People & Blogs  \n",
              "73           Youtube  00:17:23  People & Blogs  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "📌 Sample rows from channel: Al_Mokhbir_Al_Eqtisadi\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Dialogue</th>\n",
              "      <th>Type</th>\n",
              "      <th>Length</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>Al_Mokhbir_Al_Eqtisadi</td>\n",
              "      <td>المخبر_الاقتصادي__كيف_خططت_أمريكا_سرا_لتعقيم_س...</td>\n",
              "      <td>في أوائل التسعينيات الرأي العام في البرازيل\\nك...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:14:21</td>\n",
              "      <td>Entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>Al_Mokhbir_Al_Eqtisadi</td>\n",
              "      <td>المخبر_الاقتصادي__كيف_دفع_رجل_وامرأة_اقتصاد_بر...</td>\n",
              "      <td>راجل وواحدة ست\\nالاتنين المحترمين دول طلعوا في...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:21:00</td>\n",
              "      <td>Entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>Al_Mokhbir_Al_Eqtisadi</td>\n",
              "      <td>المخبر_الاقتصادي__كيف_أصبح_المغرب_حارس_بوابة_ا...</td>\n",
              "      <td>عارفين هولندا؟\\nأكيد عارفينها\\nوأكيد برضو سمعت...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:14:30</td>\n",
              "      <td>Entertainment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Channel  \\\n",
              "114  Al_Mokhbir_Al_Eqtisadi   \n",
              "175  Al_Mokhbir_Al_Eqtisadi   \n",
              "315  Al_Mokhbir_Al_Eqtisadi   \n",
              "\n",
              "                                         Episode Title  \\\n",
              "114  المخبر_الاقتصادي__كيف_خططت_أمريكا_سرا_لتعقيم_س...   \n",
              "175  المخبر_الاقتصادي__كيف_دفع_رجل_وامرأة_اقتصاد_بر...   \n",
              "315  المخبر_الاقتصادي__كيف_أصبح_المغرب_حارس_بوابة_ا...   \n",
              "\n",
              "                                       Original Script Processed Script  \\\n",
              "114  في أوائل التسعينيات الرأي العام في البرازيل\\nك...                    \n",
              "175  راجل وواحدة ست\\nالاتنين المحترمين دول طلعوا في...                    \n",
              "315  عارفين هولندا؟\\nأكيد عارفينها\\nوأكيد برضو سمعت...                    \n",
              "\n",
              "    Dialogue     Type    Length       Category  \n",
              "114           Youtube  00:14:21  Entertainment  \n",
              "175           Youtube  00:21:00  Entertainment  \n",
              "315           Youtube  00:14:30  Entertainment  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def sample_rows_per_channel(df, channel_column=\"Channel\", num_samples=3):\n",
        "    unique_channels = df[channel_column].unique()\n",
        "    for channel in unique_channels:\n",
        "        print(f\"📌 Sample rows from channel: {channel}\")\n",
        "        display(df[df[channel_column] == channel].sample(min(num_samples, len(df[df[channel_column] == channel]))))\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "print(\"📍 Checking Unlabelled DataFrame:\")\n",
        "sample_rows_per_channel(unlabelled_df)\n",
        "\n",
        "print(\"\\n📍 Checking Labelled DataFrame:\")\n",
        "sample_rows_per_channel(labelled_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis & Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Analysis on Original Script\n",
        "\t\n",
        "\tChannel Analysis:\n",
        "\t- avg length / duration of episodes per channel\n",
        "\t- modal categories per channel, Category / class imbalance\n",
        "\t- etc\n",
        "\t- metdata analysis\n",
        "\t- Text Charactertistics (formal / informal... dialogue?)\n",
        "\t\n",
        "\tNLP Analysis:\n",
        "\t- Text Sampling\n",
        "\t- Text Statistics (word count, sentence length...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk import tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def arabic_tokenizer(text):\n",
        "    return tokenize.word_tokenize(text)\n",
        "\n",
        "# Combine Arabic + English stopwords\n",
        "ALL_STOPWORDS = set(stopwords.words(\"arabic\") + stopwords.words(\"english\"))\n",
        "\n",
        "def analyze_channel_data(labelled_df):\n",
        "    print(\"=== CHANNEL ANALYSIS ===\")\n",
        "\n",
        "    df = labelled_df.copy()\n",
        "    df = df.dropna(subset=[\"Category\"])\n",
        "    df[\"Script Length\"] = df[\"Original Script\"].apply(lambda x: len(arabic_tokenizer(x)))\n",
        "\n",
        "    # Average Script length per channel\n",
        "    avg_length = df.groupby(\"Channel\")[\"Script Length\"].mean()\n",
        "    print(\"\\n📌 Average Episode Length per Channel:\\n\", avg_length)\n",
        "\n",
        "    # Number of episodes per channel\n",
        "    episode_count = df.groupby(\"Channel\")[\"Episode Title\"].nunique()\n",
        "    print(\"\\n📌 Number of Episodes per Channel:\\n\", episode_count)\n",
        "\n",
        "    # Most common category per channel\n",
        "    modal_category = df.groupby(\"Channel\")[\"Category\"].agg(lambda x: x.value_counts().idxmax() if x.value_counts().size > 0 else \"Unknown\")\n",
        "    print(\"\\n📌 Most Common Category per Channel:\\n\", modal_category)\n",
        "\n",
        "    # Plot category distribution\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    category_distribution = df[\"Category\"].value_counts()\n",
        "    sns.barplot(x=category_distribution.index, y=category_distribution.values, palette=\"viridis\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title(\"Category Distribution\")\n",
        "    plt.show()\n",
        "\n",
        "    # Dialogue analysis\n",
        "    if \"Dialogue\" in df.columns and df[\"Dialogue\"].notna().sum() > 0:\n",
        "        dialogue_counts = df[\"Dialogue\"].dropna().value_counts()\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sns.barplot(x=dialogue_counts.index, y=dialogue_counts.values, palette=\"coolwarm\")\n",
        "        plt.title(\"Dialogue vs. Non-Dialogue Distribution\")\n",
        "        plt.show()\n",
        "\n",
        "def analyze_nlp_statistics(labelled_df):\n",
        "    print(\"=== NLP ANALYSIS ===\")\n",
        "\n",
        "    df = labelled_df.copy()\n",
        "    df[\"Word Count\"] = df[\"Original Script\"].apply(lambda x: len(arabic_tokenizer(x)))\n",
        "    print(\"\\n📌 Word Count Statistics:\\n\", df[\"Word Count\"].describe())\n",
        "\n",
        "    # Lexical diversity\n",
        "    all_words = [word.lower() for sentence in df[\"Original Script\"] for word in arabic_tokenizer(sentence)]\n",
        "    unique_words = set(all_words)\n",
        "    lexical_diversity = len(unique_words) / len(all_words) if all_words else 0\n",
        "    print(\"\\n📌 Lexical Diversity:\", round(lexical_diversity, 4))\n",
        "\n",
        "    # Stopword ratio\n",
        "    filtered_words = [word for word in all_words if word not in ALL_STOPWORDS]\n",
        "    stopword_ratio = 1 - (len(filtered_words) / len(all_words)) if all_words else 0\n",
        "    print(\"\\n📌 Percentage of Stopwords:\", round(stopword_ratio * 100, 2), \"%\")\n",
        "\n",
        "    # Most common words\n",
        "    common_words = Counter(filtered_words)\n",
        "    print(\"\\n📌 Most Common Words:\\n\", common_words.most_common(10))\n",
        "\n",
        "    # Common bigrams and trigrams\n",
        "    bigrams = list(ngrams(filtered_words, 2))\n",
        "    trigrams = list(ngrams(filtered_words, 3))\n",
        "    print(\"\\n📌 Most Common Bigrams:\\n\", Counter(bigrams).most_common(10))\n",
        "    print(\"\\n📌 Most Common Trigrams:\\n\", Counter(trigrams).most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Abbreviations Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Labelled Dataset ===\n",
            "\n",
            "📌 Channel: Al_Mokhbir_Al_Eqtisadi\n",
            "Total Unique Abbreviations: 91\n",
            "Abbreviations & Counts:\n",
            "CIA: 1 times\n",
            "USAID: 1 times\n",
            "FOMC: 5 times\n",
            "IOS: 1 times\n",
            "CFA: 22 times\n",
            "SDECE: 1 times\n",
            "GSM: 2 times\n",
            "ETLA: 2 times\n",
            "BASF: 1 times\n",
            "ISO: 3 times\n",
            "AEPR: 3 times\n",
            "AK: 1 times\n",
            "NNPC: 1 times\n",
            "DGB: 1 times\n",
            "AG: 3 times\n",
            "GMBH: 1 times\n",
            "FBI: 1 times\n",
            "IQ: 2 times\n",
            "FOMO: 1 times\n",
            "SE: 1 times\n",
            "SAMA: 1 times\n",
            "SDR: 3 times\n",
            "ARAMCO: 1 times\n",
            "LNG: 1 times\n",
            "BIS: 1 times\n",
            "NUDT: 1 times\n",
            "EDA: 4 times\n",
            "ASML: 61 times\n",
            "FDPR: 1 times\n",
            "SMIC: 2 times\n",
            "DRAM: 1 times\n",
            "NAND: 2 times\n",
            "CCD: 1 times\n",
            "MH: 1 times\n",
            "SES: 1 times\n",
            "MSNBC: 1 times\n",
            "ABS: 1 times\n",
            "CEF: 1 times\n",
            "AA: 1 times\n",
            "OPC: 10 times\n",
            "DUV: 6 times\n",
            "EUV: 13 times\n",
            "SK: 1 times\n",
            "CFIUS: 3 times\n",
            "DOI: 2 times\n",
            "GIC: 3 times\n",
            "ITRI: 1 times\n",
            "TSMC: 5 times\n",
            "CSET: 1 times\n",
            "API: 2 times\n",
            "KPMG: 1 times\n",
            "CII: 1 times\n",
            "BYD: 64 times\n",
            "PHEV: 1 times\n",
            "NA: 1 times\n",
            "NBER: 1 times\n",
            "FTX: 63 times\n",
            "US: 2 times\n",
            "BUSD: 1 times\n",
            "FTT: 21 times\n",
            "AIG: 1 times\n",
            "CRP: 2 times\n",
            "ATM: 1 times\n",
            "CNC: 1 times\n",
            "NDTV: 1 times\n",
            "ERM: 2 times\n",
            "YCC: 2 times\n",
            "ENN: 1 times\n",
            "YARA: 1 times\n",
            "ECO: 1 times\n",
            "SEC: 1 times\n",
            "CHIPS: 1 times\n",
            "CCIEE: 1 times\n",
            "WS: 1 times\n",
            "MD: 2 times\n",
            "FAA: 1 times\n",
            "EASA: 1 times\n",
            "CAAC: 1 times\n",
            "UTC: 1 times\n",
            "FACC: 1 times\n",
            "CFM: 1 times\n",
            "SA: 1 times\n",
            "AVIC: 1 times\n",
            "TFR: 1 times\n",
            "OCP: 8 times\n",
            "IATA: 1 times\n",
            "VTB: 1 times\n",
            "MOEX: 1 times\n",
            "PRF: 2 times\n",
            "ESSF: 2 times\n",
            "ISIF: 1 times\n",
            "\n",
            "📌 Channel: B Hodoo2\n",
            "Total Unique Abbreviations: 0\n",
            "Abbreviations & Counts:\n",
            "\n",
            "📌 Channel: Business Bel Araby\n",
            "Total Unique Abbreviations: 0\n",
            "Abbreviations & Counts:\n",
            "\n",
            "📌 Channel: Fi_Al_Hadaraa\n",
            "Total Unique Abbreviations: 5\n",
            "Abbreviations & Counts:\n",
            "BM: 3 times\n",
            "CV: 1 times\n",
            "CEO: 1 times\n",
            "GDB: 1 times\n",
            "ICD: 1 times\n",
            "\n",
            "📌 Channel: Kefaya Ba2a\n",
            "Total Unique Abbreviations: 0\n",
            "Abbreviations & Counts:\n",
            "\n",
            "=== Unlabelled Dataset ===\n",
            "\n",
            "📌 Channel: Da7ee7\n",
            "Total Unique Abbreviations: 84\n",
            "Abbreviations & Counts:\n",
            "DEFCON: 4 times\n",
            "AI: 15 times\n",
            "BMI: 1 times\n",
            "MUTV: 1 times\n",
            "CTRL: 8 times\n",
            "OCD: 1 times\n",
            "AK: 1 times\n",
            "GTA: 2 times\n",
            "IB: 3 times\n",
            "IR: 1 times\n",
            "MIT: 4 times\n",
            "PACS: 4 times\n",
            "USB: 3 times\n",
            "IOS: 1 times\n",
            "IKEA: 1 times\n",
            "PR: 3 times\n",
            "NASA: 1 times\n",
            "WWWF: 1 times\n",
            "TV: 2 times\n",
            "WWE: 1 times\n",
            "IQ: 6 times\n",
            "ASL: 1 times\n",
            "ISS: 1 times\n",
            "CQD: 1 times\n",
            "SOS: 1 times\n",
            "SS: 8 times\n",
            "SHUT: 2 times\n",
            "UP: 3 times\n",
            "WORKING: 1 times\n",
            "GPS: 1 times\n",
            "TSMC: 13 times\n",
            "AMD: 1 times\n",
            "ASML: 6 times\n",
            "HOAX: 1 times\n",
            "FMRI: 1 times\n",
            "CCC: 2 times\n",
            "PDP: 1 times\n",
            "OS: 7 times\n",
            "MGM: 1 times\n",
            "CBS: 1 times\n",
            "MK: 3 times\n",
            "DC: 20 times\n",
            "BYD: 3 times\n",
            "POW: 1 times\n",
            "XX: 3 times\n",
            "XY: 2 times\n",
            "MBC: 1 times\n",
            "IV: 1 times\n",
            "LEED: 1 times\n",
            "DJ: 2 times\n",
            "SVP: 1 times\n",
            "ETF: 2 times\n",
            "IPO: 1 times\n",
            "NYU: 2 times\n",
            "DNA: 2 times\n",
            "ABC: 1 times\n",
            "ATP: 6 times\n",
            "CD: 1 times\n",
            "NOAA: 1 times\n",
            "ADHD: 1 times\n",
            "CDC: 1 times\n",
            "LSD: 3 times\n",
            "AWM: 1 times\n",
            "CV: 4 times\n",
            "HR: 6 times\n",
            "XIV: 1 times\n",
            "USA: 3 times\n",
            "CIA: 1 times\n",
            "STAR: 1 times\n",
            "DFD: 1 times\n",
            "PSE: 1 times\n",
            "FBI: 1 times\n",
            "BI: 1 times\n",
            "AA: 1 times\n",
            "JIT: 1 times\n",
            "BMW: 1 times\n",
            "GM: 1 times\n",
            "IMDB: 1 times\n",
            "PG: 1 times\n",
            "FM: 3 times\n",
            "PGMOL: 1 times\n",
            "PES: 1 times\n",
            "KFC: 1 times\n",
            "JBS: 1 times\n",
            "\n",
            "📌 Channel: Elsaha\n",
            "Total Unique Abbreviations: 0\n",
            "Abbreviations & Counts:\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def extract_abbreviations_per_channel(df, text_column=\"Original Script\", channel_column=\"Channel\"):\n",
        "    # Match words with only capital letters, min length 2\n",
        "    abbreviation_pattern = re.compile(r\"\\b[A-Z]{2,}\\b\")\n",
        "    \n",
        "    channel_abbreviation_stats = {}\n",
        "\n",
        "    for channel, group in df.groupby(channel_column):\n",
        "        all_abbreviations = []\n",
        "        \n",
        "        for text in group[text_column].dropna():\n",
        "            abbreviations = abbreviation_pattern.findall(text)\n",
        "            all_abbreviations.extend(abbreviations)\n",
        "        \n",
        "        abbreviation_counts = Counter(all_abbreviations)\n",
        "        channel_abbreviation_stats[channel] = abbreviation_counts\n",
        "    \n",
        "    return channel_abbreviation_stats\n",
        "\n",
        "# Run on both labelled and unlabelled datasets\n",
        "print(\"=== Labelled Dataset ===\")\n",
        "labelled_abbreviations = extract_abbreviations_per_channel(labelled_df)\n",
        "for channel, counts in labelled_abbreviations.items():\n",
        "    print(f\"\\n📌 Channel: {channel}\")\n",
        "    print(f\"Total Unique Abbreviations: {len(counts)}\")\n",
        "    print(\"Abbreviations & Counts:\")\n",
        "    for abbr, count in counts.items():\n",
        "        print(f\"{abbr}: {count} times\")\n",
        "\n",
        "print(\"\\n=== Unlabelled Dataset ===\")\n",
        "unlabelled_abbreviations = extract_abbreviations_per_channel(unlabelled_df)\n",
        "for channel, counts in unlabelled_abbreviations.items():\n",
        "    print(f\"\\n📌 Channel: {channel}\")\n",
        "    print(f\"Total Unique Abbreviations: {len(counts)}\")\n",
        "    print(\"Abbreviations & Counts:\")\n",
        "    for abbr, count in counts.items():\n",
        "        print(f\"{abbr}: {count} times\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Preprocessing\n",
        "\t- Punctuation, Symbols, Numbers, Diactritics (tashkeel) removal\n",
        "\t- Stemming / lemmatization?\n",
        "\t- Normalization of Arabic letters\n",
        "\t- Code switching handling? Translation? What about abbreviations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Arabic normalization map\n",
        "ARABIC_NORMALIZATION_MAP = str.maketrans({\n",
        "    \"أ\": \"ا\",\n",
        "    \"إ\": \"ا\",\n",
        "    \"آ\": \"ا\",\n",
        "    \"ى\": \"ي\",\n",
        "    \"ة\": \"ه\",\n",
        "})\n",
        "\n",
        "# Arabic punctuation characters\n",
        "ARABIC_PUNCTUATION = \"؟،؛«»ـ…“”‘’–—\"\n",
        "\n",
        "# Combined punctuation\n",
        "ALL_PUNCTUATION = string.punctuation + ARABIC_PUNCTUATION\n",
        "\n",
        "# Arabic Stopword list (can be extended)\n",
        "ARABIC_STOPWORDS = set([\n",
        "    \"في\", \"من\", \"على\", \"ما\", \"و\", \"لا\", \"عن\", \"إلى\", \"أن\", \"هو\", \"هي\", \"هذا\", \"ذلك\", \"كل\", \"كان\", \"كما\",\n",
        "    \"لذلك\", \"أو\", \"أي\", \"لم\", \"قد\", \"أكثر\", \"أقل\", \"هنا\", \"هناك\", \"بعد\", \"قبل\", \"بين\", \"مع\", \"حتى\", \"إذا\",\n",
        "    \"ثم\", \"لكن\", \"أحد\", \"أيضا\", \"أثناء\", \"عند\", \"أين\", \"كيف\", \"إلا\", \"أصبح\", \"لأن\", \"بسبب\", \"هذه\", \"هؤلاء\",\n",
        "    \"التي\", \"الذي\", \"الذين\", \"اللذي\", \"اللذين\", \"اللائي\"\n",
        "])\n",
        "\n",
        "# English abbreviations dictionary\n",
        "ENGLISH_ABBREVIATIONS = {\n",
        "    \"CIA\": \"وكالة المخابرات المركزية\",\n",
        "    \"USAID\": \"الوكالة الأمريكية للتنمية الدولية\",\n",
        "    \"FOMC\": \"اللجنة الفيدرالية للسوق المفتوحة\",\n",
        "    \"IOS\": \"نظام تشغيل آي أو إس\",\n",
        "    \"CFA\": \"محلل مالي معتمد\",\n",
        "    \"SDECE\": \"الخدمة السرية للدفاع الوطني\",\n",
        "    \"GSM\": \"النظام العالمي للاتصالات المتنقلة\",\n",
        "    \"ETLA\": \"الاتحاد الأوروبي للتكنولوجيا والتعليم\",\n",
        "    \"BASF\": \"شركة الكيميائيات الألمانية\",\n",
        "    \"ISO\": \"المنظمة الدولية للتوحيد القياسي\",\n",
        "    \"AEPR\": \"التقييم السنوي للأداء الاقتصادي\",\n",
        "    \"AK\": \"كلاشينكوف\",\n",
        "    \"NNPC\": \"شركة البترول الوطنية النيجيرية\",\n",
        "    \"DGB\": \"الإدارة العامة للأمن\",\n",
        "    \"AG\": \"شركة ذات مسؤولية محدودة\",\n",
        "    \"GMBH\": \"شركة محدودة المسؤولية في ألمانيا\",\n",
        "    \"FBI\": \"مكتب التحقيقات الفيدرالي\",\n",
        "    \"IQ\": \"معدل الذكاء\",\n",
        "    \"FOMO\": \"الخوف من تفويت الفرصة\",\n",
        "    \"SE\": \"هندسة البرمجيات\",\n",
        "    \"SAMA\": \"مؤسسة النقد العربي السعودي\",\n",
        "    \"SDR\": \"حقوق السحب الخاصة\",\n",
        "    \"ARAMCO\": \"شركة أرامكو السعودية\",\n",
        "    \"LNG\": \"الغاز الطبيعي المسال\",\n",
        "    \"BIS\": \"بنك التسويات الدولية\",\n",
        "    \"NUDT\": \"جامعة الدفاع الوطني للتكنولوجيا\",\n",
        "    \"EDA\": \"إدارة التنمية الاقتصادية\",\n",
        "    \"ASML\": \"شركة تكنولوجيا أشباه الموصلات\",\n",
        "    \"FDPR\": \"لوائح المنتجات الأجنبية المباشرة\",\n",
        "    \"SMIC\": \"شركة تصنيع أشباه الموصلات الصينية\",\n",
        "    \"DRAM\": \"ذاكرة الوصول العشوائي الديناميكية\",\n",
        "    \"NAND\": \"ذاكرة فلاش ناند\",\n",
        "    \"CCD\": \"جهاز الشحن المزدوج\",\n",
        "    \"MH\": \"ميغاهيرتز\",\n",
        "    \"SES\": \"أنظمة الاتصالات الفضائية\",\n",
        "    \"MSNBC\": \"قناة الأخبار الأمريكية\",\n",
        "    \"ABS\": \"نظام الفرامل المانع للانغلاق\",\n",
        "    \"CEF\": \"صندوق الصرف الأوروبي\",\n",
        "    \"AA\": \"الخطوط الجوية الأمريكية\",\n",
        "    \"OPC\": \"لجنة العمليات\",\n",
        "    \"DUV\": \"الأشعة فوق البنفسجية العميقة\",\n",
        "    \"EUV\": \"الأشعة فوق البنفسجية الشديدة\",\n",
        "    \"CFIUS\": \"لجنة الاستثمار الأجنبي في الولايات المتحدة\",\n",
        "    \"DOI\": \"وزارة الداخلية\",\n",
        "    \"GIC\": \"شركة الاستثمار الحكومية\",\n",
        "    \"ITRI\": \"معهد الأبحاث الصناعية\",\n",
        "    \"TSMC\": \"شركة تصنيع أشباه الموصلات التايوانية\",\n",
        "    \"API\": \"واجهة برمجة التطبيقات\",\n",
        "    \"KPMG\": \"شركة التدقيق العالمية\",\n",
        "    \"CII\": \"اتحاد الصناعات الهندية\",\n",
        "    \"BYD\": \"شركة السيارات الكهربائية الصينية\",\n",
        "    \"PHEV\": \"السيارات الكهربائية الهجينة\",\n",
        "    \"NBER\": \"المكتب الوطني للبحوث الاقتصادية\",\n",
        "    \"FTX\": \"منصة تداول العملات الرقمية\",\n",
        "    \"US\": \"الولايات المتحدة\",\n",
        "    \"BUSD\": \"عملة بينانس المستقرة\",\n",
        "    \"FTT\": \"رمز FTX الأصلي\",\n",
        "    \"AIG\": \"مجموعة التأمين الأمريكية\",\n",
        "    \"CRP\": \"برنامج الحفظ الزراعي\",\n",
        "    \"ATM\": \"الصراف الآلي\",\n",
        "    \"CNC\": \"التحكم الرقمي بالكمبيوتر\",\n",
        "    \"NDTV\": \"القناة الهندية الإخبارية\",\n",
        "    \"ERM\": \"إدارة المخاطر المؤسسية\",\n",
        "    \"YCC\": \"التحكم في منحنى العائد\",\n",
        "    \"ENN\": \"شركة الطاقة الصينية\",\n",
        "    \"ECO\": \"النظام البيئي\",\n",
        "    \"SEC\": \"لجنة الأوراق المالية والبورصات\",\n",
        "    \"CHIPS\": \"قانون دعم إنتاج أشباه الموصلات\",\n",
        "    \"CCIEE\": \"المركز الصيني للتبادلات الاقتصادية الدولية\",\n",
        "    \"WS\": \"وول ستريت\",\n",
        "    \"MD\": \"المدير الطبي\",\n",
        "    \"FAA\": \"إدارة الطيران الفيدرالية\",\n",
        "    \"IATA\": \"الاتحاد الدولي للنقل الجوي\",\n",
        "    \"MIT\": \"معهد ماساتشوستس للتكنولوجيا\",\n",
        "    \"PACS\": \"نظام أرشفة الصور والاتصالات\",\n",
        "    \"USB\": \"منفذ الناقل التسلسلي العالمي\",\n",
        "    \"NASA\": \"وكالة الفضاء الأمريكية\",\n",
        "    \"WWE\": \"المصارعة العالمية الترفيهية\",\n",
        "    \"GPS\": \"نظام تحديد المواقع العالمي\"\n",
        "}\n",
        "\n",
        "import qalsadi.lemmatizer\n",
        "import pyarabic.araby as araby\n",
        "\n",
        "# Initialize Qalsadi lemmatizer\n",
        "lemmatizer = qalsadi.lemmatizer.Lemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"\"\n",
        "\n",
        "    # Replace newline characters with space\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Normalize Arabic letters\n",
        "    text = text.translate(ARABIC_NORMALIZATION_MAP)\n",
        "\n",
        "    # Replace English abbreviations\n",
        "    words = text.split()\n",
        "    replaced_words = []\n",
        "    for word in words:\n",
        "        if re.match(r\"^[A-Z]{2,}$\", word):\n",
        "            word = ENGLISH_ABBREVIATIONS.get(word.upper(), word)\n",
        "        replaced_words.append(word)\n",
        "    text = \" \".join(replaced_words)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{re.escape(ALL_PUNCTUATION)}]\", \" \", text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [tok for tok in tokens if tok not in ARABIC_STOPWORDS]\n",
        "\n",
        "    # Apply lemmatization using Qalsadi\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(tok) for tok in filtered_tokens]\n",
        "\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "for df in [unlabelled_df, labelled_df]:\n",
        "    df[\"Processed Script\"] = df[\"Original Script\"].apply(preprocess_text)\n",
        "\n",
        "print(\"✅ Preprocessing completed (NLTK tokenization, stopwords, punctuation, newline handled).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sherifahammoud/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sherifahammoud/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Preprocessing completed (NLTK tokenization, stopwords, punctuation, newline handled).\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "ARABIC_NORMALIZATION_MAP = str.maketrans({\n",
        "    \"أ\": \"ا\",\n",
        "    \"إ\": \"ا\",\n",
        "    \"آ\": \"ا\",\n",
        "    \"ى\": \"ي\",\n",
        "    \"ة\": \"ه\",\n",
        "})\n",
        "\n",
        "ARABIC_PUNCTUATION = \"؟،؛«»ـ…“”‘’–—\"\n",
        "\n",
        "ALL_PUNCTUATION = string.punctuation + ARABIC_PUNCTUATION\n",
        "\n",
        "ARABIC_STOPWORDS = set([\n",
        "    \"في\", \"من\", \"على\", \"ما\", \"و\", \"لا\", \"عن\", \"إلى\", \"أن\", \"هو\", \"هي\", \"هذا\", \"ذلك\", \"كل\", \"كان\", \"كما\",\n",
        "    \"لذلك\", \"أو\", \"أي\", \"لم\", \"قد\", \"أكثر\", \"أقل\", \"هنا\", \"هناك\", \"بعد\", \"قبل\", \"بين\", \"مع\", \"حتى\", \"إذا\",\n",
        "    \"ثم\", \"لكن\", \"أحد\", \"أيضا\", \"أثناء\", \"عند\", \"أين\", \"كيف\", \"إلا\", \"أصبح\", \"لأن\", \"بسبب\", \"هذه\", \"هؤلاء\",\n",
        "    \"التي\", \"الذي\", \"الذين\", \"اللذي\", \"اللذين\", \"اللائي\"\n",
        "])\n",
        "\n",
        "ENGLISH_ABBREVIATIONS = {\n",
        "    \"CIA\": \"وكالة المخابرات المركزية\",\n",
        "    \"USAID\": \"الوكالة الأمريكية للتنمية الدولية\",\n",
        "    \"FOMC\": \"اللجنة الفيدرالية للسوق المفتوحة\",\n",
        "    \"IOS\": \"نظام تشغيل آي أو إس\",\n",
        "    \"CFA\": \"محلل مالي معتمد\",\n",
        "    \"SDECE\": \"الخدمة السرية للدفاع الوطني\",\n",
        "    \"GSM\": \"النظام العالمي للاتصالات المتنقلة\",\n",
        "    \"ETLA\": \"الاتحاد الأوروبي للتكنولوجيا والتعليم\",\n",
        "    \"BASF\": \"شركة الكيميائيات الألمانية\",\n",
        "    \"ISO\": \"المنظمة الدولية للتوحيد القياسي\",\n",
        "    \"AEPR\": \"التقييم السنوي للأداء الاقتصادي\",\n",
        "    \"AK\": \"كلاشينكوف\",\n",
        "    \"NNPC\": \"شركة البترول الوطنية النيجيرية\",\n",
        "    \"DGB\": \"الإدارة العامة للأمن\",\n",
        "    \"AG\": \"شركة ذات مسؤولية محدودة\",\n",
        "    \"GMBH\": \"شركة محدودة المسؤولية في ألمانيا\",\n",
        "    \"FBI\": \"مكتب التحقيقات الفيدرالي\",\n",
        "    \"IQ\": \"معدل الذكاء\",\n",
        "    \"FOMO\": \"الخوف من تفويت الفرصة\",\n",
        "    \"SE\": \"هندسة البرمجيات\",\n",
        "    \"SAMA\": \"مؤسسة النقد العربي السعودي\",\n",
        "    \"SDR\": \"حقوق السحب الخاصة\",\n",
        "    \"ARAMCO\": \"شركة أرامكو السعودية\",\n",
        "    \"LNG\": \"الغاز الطبيعي المسال\",\n",
        "    \"BIS\": \"بنك التسويات الدولية\",\n",
        "    \"NUDT\": \"جامعة الدفاع الوطني للتكنولوجيا\",\n",
        "    \"EDA\": \"إدارة التنمية الاقتصادية\",\n",
        "    \"ASML\": \"شركة تكنولوجيا أشباه الموصلات\",\n",
        "    \"FDPR\": \"لوائح المنتجات الأجنبية المباشرة\",\n",
        "    \"SMIC\": \"شركة تصنيع أشباه الموصلات الصينية\",\n",
        "    \"DRAM\": \"ذاكرة الوصول العشوائي الديناميكية\",\n",
        "    \"NAND\": \"ذاكرة فلاش ناند\",\n",
        "    \"CCD\": \"جهاز الشحن المزدوج\",\n",
        "    \"MH\": \"ميغاهيرتز\",\n",
        "    \"SES\": \"أنظمة الاتصالات الفضائية\",\n",
        "    \"MSNBC\": \"قناة الأخبار الأمريكية\",\n",
        "    \"ABS\": \"نظام الفرامل المانع للانغلاق\",\n",
        "    \"CEF\": \"صندوق الصرف الأوروبي\",\n",
        "    \"AA\": \"الخطوط الجوية الأمريكية\",\n",
        "    \"OPC\": \"لجنة العمليات\",\n",
        "    \"DUV\": \"الأشعة فوق البنفسجية العميقة\",\n",
        "    \"EUV\": \"الأشعة فوق البنفسجية الشديدة\",\n",
        "    \"CFIUS\": \"لجنة الاستثمار الأجنبي في الولايات المتحدة\",\n",
        "    \"DOI\": \"وزارة الداخلية\",\n",
        "    \"GIC\": \"شركة الاستثمار الحكومية\",\n",
        "    \"ITRI\": \"معهد الأبحاث الصناعية\",\n",
        "    \"TSMC\": \"شركة تصنيع أشباه الموصلات التايوانية\",\n",
        "    \"API\": \"واجهة برمجة التطبيقات\",\n",
        "    \"KPMG\": \"شركة التدقيق العالمية\",\n",
        "    \"CII\": \"اتحاد الصناعات الهندية\",\n",
        "    \"BYD\": \"شركة السيارات الكهربائية الصينية\",\n",
        "    \"PHEV\": \"السيارات الكهربائية الهجينة\",\n",
        "    \"NBER\": \"المكتب الوطني للبحوث الاقتصادية\",\n",
        "    \"FTX\": \"منصة تداول العملات الرقمية\",\n",
        "    \"US\": \"الولايات المتحدة\",\n",
        "    \"BUSD\": \"عملة بينانس المستقرة\",\n",
        "    \"FTT\": \"رمز FTX الأصلي\",\n",
        "    \"AIG\": \"مجموعة التأمين الأمريكية\",\n",
        "    \"CRP\": \"برنامج الحفظ الزراعي\",\n",
        "    \"ATM\": \"الصراف الآلي\",\n",
        "    \"CNC\": \"التحكم الرقمي بالكمبيوتر\",\n",
        "    \"NDTV\": \"القناة الهندية الإخبارية\",\n",
        "    \"ERM\": \"إدارة المخاطر المؤسسية\",\n",
        "    \"YCC\": \"التحكم في منحنى العائد\",\n",
        "    \"ENN\": \"شركة الطاقة الصينية\",\n",
        "    \"ECO\": \"النظام البيئي\",\n",
        "    \"SEC\": \"لجنة الأوراق المالية والبورصات\",\n",
        "    \"CHIPS\": \"قانون دعم إنتاج أشباه الموصلات\",\n",
        "    \"CCIEE\": \"المركز الصيني للتبادلات الاقتصادية الدولية\",\n",
        "    \"WS\": \"وول ستريت\",\n",
        "    \"MD\": \"المدير الطبي\",\n",
        "    \"FAA\": \"إدارة الطيران الفيدرالية\",\n",
        "    \"IATA\": \"الاتحاد الدولي للنقل الجوي\",\n",
        "    \"MIT\": \"معهد ماساتشوستس للتكنولوجيا\",\n",
        "    \"PACS\": \"نظام أرشفة الصور والاتصالات\",\n",
        "    \"USB\": \"منفذ الناقل التسلسلي العالمي\",\n",
        "    \"NASA\": \"وكالة الفضاء الأمريكية\",\n",
        "    \"WWE\": \"المصارعة العالمية الترفيهية\",\n",
        "    \"GPS\": \"نظام تحديد المواقع العالمي\"\n",
        "}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"\"\n",
        "\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Normalize Arabic letters\n",
        "    text = text.translate(ARABIC_NORMALIZATION_MAP)\n",
        "\n",
        "    # Replace English abbreviations\n",
        "    words = text.split()\n",
        "    replaced_words = []\n",
        "    for word in words:\n",
        "        if re.match(r\"^[A-Z]{2,}$\", word):\n",
        "            word = ENGLISH_ABBREVIATIONS.get(word.upper(), word)\n",
        "        replaced_words.append(word)\n",
        "    text = \" \".join(replaced_words)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{re.escape(ALL_PUNCTUATION)}]\", \" \", text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [tok for tok in tokens if tok not in ARABIC_STOPWORDS]\n",
        "\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "for df in [unlabelled_df, labelled_df]:\n",
        "    df[\"Processed Script\"] = df[\"Original Script\"].apply(preprocess_text)\n",
        "\n",
        "print(\"✅ Preprocessing completed (NLTK tokenization, stopwords, punctuation, newline handled).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Analysis (On Proc) 1\n",
        "\t- Text Sampling\n",
        "\t- Text Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📍 Checking Unlabelled DataFrame:\n",
            "📌 Sample rows from channel: Da7ee7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>EpisodeTitle</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>Da7ee7</td>\n",
              "      <td>كلوب  الدحيح</td>\n",
              "      <td>\"كلوب\"، حبيبي،\\nاحنا عايزين مصلحتك،\\nانت لازم ...</td>\n",
              "      <td>كلوب حبيبي احنا عايزين مصلحتك انت لازم تعتزل و...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>Da7ee7</td>\n",
              "      <td>ماركو بولو  الدحيح</td>\n",
              "      <td>ها يا \"ماركو بولو\"،\\nبقالك سنين بتلفّ حوالين ا...</td>\n",
              "      <td>ها يا ماركو بولو بقالك سنين بتلفّ حوالين العال...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>Da7ee7</td>\n",
              "      <td>كوكب القرود  الدحيح</td>\n",
              "      <td>ها يا إنسان؟\\nعاجبك كدا منظرك؟\\nياه!\\nحرام علي...</td>\n",
              "      <td>ها يا انسان عاجبك كدا منظرك ياه حرام عليكم يا ...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Channel         EpisodeTitle  \\\n",
              "71   Da7ee7         كلوب  الدحيح   \n",
              "117  Da7ee7   ماركو بولو  الدحيح   \n",
              "135  Da7ee7  كوكب القرود  الدحيح   \n",
              "\n",
              "                                       Original Script  \\\n",
              "71   \"كلوب\"، حبيبي،\\nاحنا عايزين مصلحتك،\\nانت لازم ...   \n",
              "117  ها يا \"ماركو بولو\"،\\nبقالك سنين بتلفّ حوالين ا...   \n",
              "135  ها يا إنسان؟\\nعاجبك كدا منظرك؟\\nياه!\\nحرام علي...   \n",
              "\n",
              "                                      Processed Script Category  \n",
              "71   كلوب حبيبي احنا عايزين مصلحتك انت لازم تعتزل و...           \n",
              "117  ها يا ماركو بولو بقالك سنين بتلفّ حوالين العال...           \n",
              "135  ها يا انسان عاجبك كدا منظرك ياه حرام عليكم يا ...           "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "📌 Sample rows from channel: Elsaha\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>EpisodeTitle</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Elsaha</td>\n",
              "      <td>توثيق المطبخ المصري.. حكاية سميرة عبدالقادر</td>\n",
              "      <td>احنا 7000 سنه لا مش عايزين شعرات احنا بس\\nمترب...</td>\n",
              "      <td>احنا 7000 سنه مش عايزين شعرات احنا بس متربين ع...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>Elsaha</td>\n",
              "      <td>البحث عن قاتل أمي.. حكاية نادية مبروك</td>\n",
              "      <td>كان مؤلم قوي ان استرجع شعور امي بعد ما\\nضربت ب...</td>\n",
              "      <td>مؤلم قوي ان استرجع شعور امي ضربت بالنار واحد ث...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>Elsaha</td>\n",
              "      <td>طب القوات المسلحة.. حكاية ميدالية ذهبية</td>\n",
              "      <td>فكره بتاعتنا اللي انت بتقدم زي كده او\\nشريحه ا...</td>\n",
              "      <td>فكره بتاعتنا اللي انت بتقدم زي كده او شريحه ال...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Channel                                 EpisodeTitle  \\\n",
              "167  Elsaha  توثيق المطبخ المصري.. حكاية سميرة عبدالقادر   \n",
              "191  Elsaha        البحث عن قاتل أمي.. حكاية نادية مبروك   \n",
              "176  Elsaha      طب القوات المسلحة.. حكاية ميدالية ذهبية   \n",
              "\n",
              "                                       Original Script  \\\n",
              "167  احنا 7000 سنه لا مش عايزين شعرات احنا بس\\nمترب...   \n",
              "191  كان مؤلم قوي ان استرجع شعور امي بعد ما\\nضربت ب...   \n",
              "176  فكره بتاعتنا اللي انت بتقدم زي كده او\\nشريحه ا...   \n",
              "\n",
              "                                      Processed Script Category  \n",
              "167  احنا 7000 سنه مش عايزين شعرات احنا بس متربين ع...           \n",
              "191  مؤلم قوي ان استرجع شعور امي ضربت بالنار واحد ث...           \n",
              "176  فكره بتاعتنا اللي انت بتقدم زي كده او شريحه ال...           "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "\n",
            "📍 Checking Labelled DataFrame:\n",
            "📌 Sample rows from channel: Kefaya Ba2a\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Dialogue</th>\n",
              "      <th>Type</th>\n",
              "      <th>Length</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Kefaya Ba2a</td>\n",
              "      <td>بودكاست كفاية بقى - بطاطس مش محمرة</td>\n",
              "      <td>اهلا بكم في بودكاست كفايه بقى البودكاست\\nده يا...</td>\n",
              "      <td>اهلا بكم بودكاست كفايه بقي البودكاست ده يا جما...</td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Kefaya Ba2a</td>\n",
              "      <td>بودكاست كفاية بقى “بالله عليك” قسم وسمعني ٢</td>\n",
              "      <td>اهلا بكم في بودكاست كفايه بقى النسخه\\nالرمضاني...</td>\n",
              "      <td>اهلا بكم بودكاست كفايه بقي النسخه الرمضانيه ال...</td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Kefaya Ba2a</td>\n",
              "      <td>بودكاست كفاية بقى “بالله عليك” - العسف الرمضاني</td>\n",
              "      <td>اهلا بكم في بودكاست كفايه بقى النسخه\\nالرمضاني...</td>\n",
              "      <td>اهلا بكم بودكاست كفايه بقي النسخه الرمضانيه ال...</td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Channel                                    Episode Title  \\\n",
              "12  Kefaya Ba2a               بودكاست كفاية بقى - بطاطس مش محمرة   \n",
              "18  Kefaya Ba2a      بودكاست كفاية بقى “بالله عليك” قسم وسمعني ٢   \n",
              "16  Kefaya Ba2a  بودكاست كفاية بقى “بالله عليك” - العسف الرمضاني   \n",
              "\n",
              "                                      Original Script  \\\n",
              "12  اهلا بكم في بودكاست كفايه بقى البودكاست\\nده يا...   \n",
              "18  اهلا بكم في بودكاست كفايه بقى النسخه\\nالرمضاني...   \n",
              "16  اهلا بكم في بودكاست كفايه بقى النسخه\\nالرمضاني...   \n",
              "\n",
              "                                     Processed Script Dialogue     Type  \\\n",
              "12  اهلا بكم بودكاست كفايه بقي البودكاست ده يا جما...    false  Podcast   \n",
              "18  اهلا بكم بودكاست كفايه بقي النسخه الرمضانيه ال...    false  Podcast   \n",
              "16  اهلا بكم بودكاست كفايه بقي النسخه الرمضانيه ال...    false  Podcast   \n",
              "\n",
              "   Length        Category  \n",
              "12         People & Blogs  \n",
              "18         People & Blogs  \n",
              "16         People & Blogs  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "📌 Sample rows from channel: Business Bel Araby\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Dialogue</th>\n",
              "      <th>Type</th>\n",
              "      <th>Length</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Business Bel Araby</td>\n",
              "      <td>بناء المشروعات من البداية الى النجاح مع عمر اب...</td>\n",
              "      <td>لا هو رقم واحد في القاعده رقم واحد يعني\\nالنفس...</td>\n",
              "      <td>رقم واحد القاعده رقم واحد يعني النفس الطويل يك...</td>\n",
              "      <td>true</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>Education</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Business Bel Araby</td>\n",
              "      <td>من مهندس الى رئيس مجلس ادارة لشركة مقاولات - ط...</td>\n",
              "      <td>اوحش حاجه تعملها لنفسك ان انت وانت نازل\\nالشغل...</td>\n",
              "      <td>اوحش حاجه تعملها لنفسك ان انت وانت نازل الشغل ...</td>\n",
              "      <td>true</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Business Bel Araby</td>\n",
              "      <td>اسرا البيع الاونلاين وكيفية بناء براند قوي - م...</td>\n",
              "      <td>ودائما الناس بتستنى الوقت المثالي عشان\\nتبدا ي...</td>\n",
              "      <td>ودائما الناس بتستني الوقت المثالي عشان تبدا يع...</td>\n",
              "      <td>true</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Channel                                      Episode Title  \\\n",
              "24  Business Bel Araby  بناء المشروعات من البداية الى النجاح مع عمر اب...   \n",
              "23  Business Bel Araby  من مهندس الى رئيس مجلس ادارة لشركة مقاولات - ط...   \n",
              "21  Business Bel Araby  اسرا البيع الاونلاين وكيفية بناء براند قوي - م...   \n",
              "\n",
              "                                      Original Script  \\\n",
              "24  لا هو رقم واحد في القاعده رقم واحد يعني\\nالنفس...   \n",
              "23  اوحش حاجه تعملها لنفسك ان انت وانت نازل\\nالشغل...   \n",
              "21  ودائما الناس بتستنى الوقت المثالي عشان\\nتبدا ي...   \n",
              "\n",
              "                                     Processed Script Dialogue     Type  \\\n",
              "24  رقم واحد القاعده رقم واحد يعني النفس الطويل يك...     true  Podcast   \n",
              "23  اوحش حاجه تعملها لنفسك ان انت وانت نازل الشغل ...     true  Podcast   \n",
              "21  ودائما الناس بتستني الوقت المثالي عشان تبدا يع...     true  Podcast   \n",
              "\n",
              "   Length        Category  \n",
              "24              Education  \n",
              "23         People & Blogs  \n",
              "21         People & Blogs  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "📌 Sample rows from channel: B Hodoo2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Dialogue</th>\n",
              "      <th>Type</th>\n",
              "      <th>Length</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>B Hodoo2</td>\n",
              "      <td>فيديو لن يعجبك!!! كلام صريح واعترافات خاصة عن ...</td>\n",
              "      <td>طبعا كالعاده احنا اتعودنا ان اي فيديو\\nيبدا لا...</td>\n",
              "      <td>طبعا كالعاده احنا اتعودنا ان اي فيديو يبدا لاز...</td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>Education</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>B Hodoo2</td>\n",
              "      <td>إزاي ضاعفت دخلي بدون شغل إضافي! - عن الرزق ووس...</td>\n",
              "      <td>انا النهارده جاي اقوللك ازاي تزود من\\nدخلك الم...</td>\n",
              "      <td>انا النهارده جاي اقوللك ازاي تزود دخلك المادي ...</td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>Education</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>B Hodoo2</td>\n",
              "      <td>مين هينقذك لو غرفت؟ _ بودكاست بهدوء مع كريم _ ...</td>\n",
              "      <td>تساؤل قريته على الفيسبوك عليه اكتر من\\n3000 شي...</td>\n",
              "      <td>تساؤل قريته علي الفيسبوك عليه اكتر 3000 شير اق...</td>\n",
              "      <td>false</td>\n",
              "      <td>Podcast</td>\n",
              "      <td></td>\n",
              "      <td>Education</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Channel                                      Episode Title  \\\n",
              "35  B Hodoo2  فيديو لن يعجبك!!! كلام صريح واعترافات خاصة عن ...   \n",
              "27  B Hodoo2  إزاي ضاعفت دخلي بدون شغل إضافي! - عن الرزق ووس...   \n",
              "48  B Hodoo2  مين هينقذك لو غرفت؟ _ بودكاست بهدوء مع كريم _ ...   \n",
              "\n",
              "                                      Original Script  \\\n",
              "35  طبعا كالعاده احنا اتعودنا ان اي فيديو\\nيبدا لا...   \n",
              "27  انا النهارده جاي اقوللك ازاي تزود من\\nدخلك الم...   \n",
              "48  تساؤل قريته على الفيسبوك عليه اكتر من\\n3000 شي...   \n",
              "\n",
              "                                     Processed Script Dialogue     Type  \\\n",
              "35  طبعا كالعاده احنا اتعودنا ان اي فيديو يبدا لاز...    false  Podcast   \n",
              "27  انا النهارده جاي اقوللك ازاي تزود دخلك المادي ...    false  Podcast   \n",
              "48  تساؤل قريته علي الفيسبوك عليه اكتر 3000 شير اق...    false  Podcast   \n",
              "\n",
              "   Length   Category  \n",
              "35         Education  \n",
              "27         Education  \n",
              "48         Education  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "📌 Sample rows from channel: Fi_Al_Hadaraa\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Dialogue</th>\n",
              "      <th>Type</th>\n",
              "      <th>Length</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>Fi_Al_Hadaraa</td>\n",
              "      <td>صديقي_الإنسان_بتحب_الصدر_ولا_الورك__في_الحضارة</td>\n",
              "      <td>طب ايه\\nالليل بيخلص وكاتب التاريخ بيغلق صفحاته...</td>\n",
              "      <td>طب ايه الليل بيخلص وكاتب التاريخ بيغلق صفحاته ...</td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:17:09</td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>Fi_Al_Hadaraa</td>\n",
              "      <td>هالاند_تريبل_كابتن__في_الحضارة</td>\n",
              "      <td>يعني خلاص؟\\nما فيش منّه تاني؟\\nما خلاص بقى يا ...</td>\n",
              "      <td>يعني خلاص فيش منّه تاني خلاص بقي يا نجم كبرنا ...</td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:24:18</td>\n",
              "      <td>Entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>Fi_Al_Hadaraa</td>\n",
              "      <td>متلازمة_المحتال__صديقي_الانسان_هل_تستحق_منصبك_...</td>\n",
              "      <td>أيوة، حاضر.\\nأيوة.\\n\"ريم\"؟!\\nإيه يا بنتي اللي ...</td>\n",
              "      <td>ايوه حاضر ايوه ريم ايه يا بنتي اللي انتي عاملا...</td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:13:07</td>\n",
              "      <td>People &amp; Blogs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Channel                                      Episode Title  \\\n",
              "72  Fi_Al_Hadaraa     صديقي_الإنسان_بتحب_الصدر_ولا_الورك__في_الحضارة   \n",
              "94  Fi_Al_Hadaraa                     هالاند_تريبل_كابتن__في_الحضارة   \n",
              "82  Fi_Al_Hadaraa  متلازمة_المحتال__صديقي_الانسان_هل_تستحق_منصبك_...   \n",
              "\n",
              "                                      Original Script  \\\n",
              "72  طب ايه\\nالليل بيخلص وكاتب التاريخ بيغلق صفحاته...   \n",
              "94  يعني خلاص؟\\nما فيش منّه تاني؟\\nما خلاص بقى يا ...   \n",
              "82  أيوة، حاضر.\\nأيوة.\\n\"ريم\"؟!\\nإيه يا بنتي اللي ...   \n",
              "\n",
              "                                     Processed Script Dialogue     Type  \\\n",
              "72  طب ايه الليل بيخلص وكاتب التاريخ بيغلق صفحاته ...           Youtube   \n",
              "94  يعني خلاص فيش منّه تاني خلاص بقي يا نجم كبرنا ...           Youtube   \n",
              "82  ايوه حاضر ايوه ريم ايه يا بنتي اللي انتي عاملا...           Youtube   \n",
              "\n",
              "      Length        Category  \n",
              "72  00:17:09  People & Blogs  \n",
              "94  00:24:18   Entertainment  \n",
              "82  00:13:07  People & Blogs  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "📌 Sample rows from channel: Al_Mokhbir_Al_Eqtisadi\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Original Script</th>\n",
              "      <th>Processed Script</th>\n",
              "      <th>Dialogue</th>\n",
              "      <th>Type</th>\n",
              "      <th>Length</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>Al_Mokhbir_Al_Eqtisadi</td>\n",
              "      <td>المخبر_الاقتصادي___لماذا_قد_ينهار_اقتصاد_أمريك...</td>\n",
              "      <td>في فبراير 2023\\nيعني بعد اربع سنين تقريبا من ا...</td>\n",
              "      <td>فبراير 2023 يعني اربع سنين تقريبا النهارده طبع...</td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:14:17</td>\n",
              "      <td>Entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>Al_Mokhbir_Al_Eqtisadi</td>\n",
              "      <td>المخبر_الاقتصادي_لماذا_ستختفي_كميات_ضخمة_من_ال...</td>\n",
              "      <td>في 31 مارس اللي فات\\nالرئيس الصيني شي جين بينغ...</td>\n",
              "      <td>31 مارس اللي فات الرئيس الصيني شي جين بينغ نشر...</td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:13:25</td>\n",
              "      <td>Entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>Al_Mokhbir_Al_Eqtisadi</td>\n",
              "      <td>المخبر_الاقتصادي__كيف_ترفع_الشركات_أسعار_المنت...</td>\n",
              "      <td>في اواخر 2016 حصل هجوم كبير من\\nالمستهلكين في ...</td>\n",
              "      <td>اواخر 2016 حصل هجوم كبير المستهلكين بريطانيا ع...</td>\n",
              "      <td></td>\n",
              "      <td>Youtube</td>\n",
              "      <td>00:15:12</td>\n",
              "      <td>Entertainment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Channel  \\\n",
              "166  Al_Mokhbir_Al_Eqtisadi   \n",
              "293  Al_Mokhbir_Al_Eqtisadi   \n",
              "263  Al_Mokhbir_Al_Eqtisadi   \n",
              "\n",
              "                                         Episode Title  \\\n",
              "166  المخبر_الاقتصادي___لماذا_قد_ينهار_اقتصاد_أمريك...   \n",
              "293  المخبر_الاقتصادي_لماذا_ستختفي_كميات_ضخمة_من_ال...   \n",
              "263  المخبر_الاقتصادي__كيف_ترفع_الشركات_أسعار_المنت...   \n",
              "\n",
              "                                       Original Script  \\\n",
              "166  في فبراير 2023\\nيعني بعد اربع سنين تقريبا من ا...   \n",
              "293  في 31 مارس اللي فات\\nالرئيس الصيني شي جين بينغ...   \n",
              "263  في اواخر 2016 حصل هجوم كبير من\\nالمستهلكين في ...   \n",
              "\n",
              "                                      Processed Script Dialogue     Type  \\\n",
              "166  فبراير 2023 يعني اربع سنين تقريبا النهارده طبع...           Youtube   \n",
              "293  31 مارس اللي فات الرئيس الصيني شي جين بينغ نشر...           Youtube   \n",
              "263  اواخر 2016 حصل هجوم كبير المستهلكين بريطانيا ع...           Youtube   \n",
              "\n",
              "       Length       Category  \n",
              "166  00:14:17  Entertainment  \n",
              "293  00:13:25  Entertainment  \n",
              "263  00:15:12  Entertainment  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def sample_rows_per_channel(df, channel_column=\"Channel\", num_samples=3):\n",
        "    unique_channels = df[channel_column].unique()\n",
        "    for channel in unique_channels:\n",
        "        print(f\"📌 Sample rows from channel: {channel}\")\n",
        "        display(df[df[channel_column] == channel].sample(min(num_samples, len(df[df[channel_column] == channel]))))\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "print(\"📍 Checking Unlabelled DataFrame:\")\n",
        "sample_rows_per_channel(unlabelled_df)\n",
        "\n",
        "print(\"\\n📍 Checking Labelled DataFrame:\")\n",
        "sample_rows_per_channel(labelled_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Data Preparation\n",
        "\t- Category selection? Handle Class Imbalance\n",
        "\t- Feature extraction (bag of words, TF-IDF, word embeddings / vectorization)\n",
        "\t- Encoding\n",
        "\t- Data splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
            "Collecting imbalanced-learn (from imblearn)\n",
            "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
            "Installing collected packages: imbalanced-learn, imblearn\n",
            "Successfully installed imbalanced-learn-0.12.4 imblearn-0.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Category distribution:\n",
            "Category\n",
            "Entertainment     229\n",
            "People & Blogs     72\n",
            "Education          24\n",
            "Comedy              4\n",
            "                    2\n",
            "Name: count, dtype: int64\n",
            "Original dataset size: 331\n",
            "Filtered dataset size: 325\n",
            "\n",
            "Category encoding mapping:\n",
            "Education -> 0\n",
            "Entertainment -> 1\n",
            "People & Blogs -> 2\n",
            "\n",
            "Class distribution before SMOTE:\n",
            "Counter({1: 183, 2: 58, 0: 19})\n",
            "\n",
            "Class distribution after SMOTE:\n",
            "Counter({1: 183, 2: 183, 0: 183})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sherifahammoud/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training set shape: (549, 34982)\n",
            "Testing set shape: (65, 34982)\n",
            "Unlabelled data shape: (193, 34982)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Category Selection and Class Imbalance Handling\n",
        "print(\"Category distribution:\")\n",
        "category_counts = labelled_df['Category'].value_counts()\n",
        "print(category_counts)\n",
        "\n",
        "# Handling class imbalance options:\n",
        "# a. Remove categories with very few samples (optional)\n",
        "MIN_SAMPLES = 5\n",
        "valid_categories = category_counts[category_counts >=\n",
        "                                   MIN_SAMPLES].index.tolist()\n",
        "filtered_df = labelled_df[labelled_df['Category'].isin(valid_categories)]\n",
        "\n",
        "print(f\"Original dataset size: {len(labelled_df)}\")\n",
        "print(f\"Filtered dataset size: {len(filtered_df)}\")\n",
        "\n",
        "# 2. Feature Extraction\n",
        "# Choose one of the following feature extraction methods:\n",
        "\n",
        "# Option 1: Bag of Words\n",
        "count_vectorizer = CountVectorizer(min_df=2, max_df=0.95)\n",
        "X_bow = count_vectorizer.fit_transform(filtered_df['Processed Script'])\n",
        "\n",
        "# Option 2: TF-IDF (usually performs better for text classification)\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.95)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(filtered_df['Processed Script'])\n",
        "\n",
        "# Let's use TF-IDF for our primary feature set\n",
        "X = X_tfidf\n",
        "\n",
        "# 3. Encoding the target variable (categories)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(filtered_df['Category'])\n",
        "\n",
        "# Display the encoding mapping\n",
        "category_mapping = dict(\n",
        "    zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
        "print(\"\\nCategory encoding mapping:\")\n",
        "for category, code in category_mapping.items():\n",
        "    print(f\"{category} -> {code}\")\n",
        "\n",
        "# 4. Data Splitting\n",
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 5. Further handle class imbalance with SMOTE (only on training data)\n",
        "print(\"\\nClass distribution before SMOTE:\")\n",
        "print(Counter(y_train))\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nClass distribution after SMOTE:\")\n",
        "print(Counter(y_train_resampled))\n",
        "\n",
        "# 6. Prepare a function to process and vectorize unlabelled data\n",
        "def prepare_unlabelled_data(unlabelled_df, vectorizer):\n",
        "    \"\"\"Process and vectorize unlabelled data using the same vectorizer as the training data\"\"\"\n",
        "    X_unlabelled = vectorizer.transform(unlabelled_df['Processed Script'])\n",
        "    return X_unlabelled\n",
        "\n",
        "\n",
        "# Process unlabelled data using the same vectorizer\n",
        "X_unlabelled = prepare_unlabelled_data(unlabelled_df, tfidf_vectorizer)\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train_resampled.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "print(f\"Unlabelled data shape: {X_unlabelled.shape}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
